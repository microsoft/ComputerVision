{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Image Classification Model\n",
    "\n",
    "In this notebook, we give an introduction to training an image classification model using [fast.ai](https://www.fast.ai/). Using a small dataset of four different beverage packages, we demonstrate training and evaluating a CNN image classification model. We also cover one of the most common ways to store data on a file system for this type of problem.\n",
    "\n",
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure edits to libraries are loaded and plotting is shown in the notebook.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we import all functions in `fastai` (`from fastai.vision import *`) to simplify using different utilies provided by the library for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "\n",
    "# fastai and torch\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.metrics import accuracy\n",
    "\n",
    "# local modules\n",
    "from utils_cv.classification.model import TrainMetricsRecorder\n",
    "from utils_cv.classification.plot import plot_pr_roc_curves\n",
    "from utils_cv.classification.widget import ResultsWidget\n",
    "from utils_cv.classification.data import Urls\n",
    "from utils_cv.common.data import unzip_url\n",
    "from utils_cv.common.gpu import which_processor\n",
    "\n",
    "print(f\"Fast.ai version = {fastai.__version__}\")\n",
    "which_processor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows your machine's GPUs (if has any) and the computing device `fastai/torch` is using. We suggest using an  [Azure DSVM](https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/) Standard NC6 for an as needed GPU compute resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set some model runtime parameters. We use the `unzip_url` helper function to download and unzip the data used in this example notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "DATA_PATH     = unzip_url(Urls.fridge_objects_path, exist_ok=True)\n",
    "EPOCHS        = 5\n",
    "LEARNING_RATE = 1e-4\n",
    "IMAGE_SIZE    = 299\n",
    "\n",
    "BATCH_SIZE    = 16\n",
    "ARCHITECTURE  = models.resnet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Prepare Image Classification Dataset\n",
    "\n",
    "In this notebook, we use a toy dataset called *Fridge Objects*, which consists of 134 images of 4 classes of beverage container `{can, carton, milk bottle, water bottle}` photos taken on different backgrounds. The helper function downloads and unzips data set to the `image_classification/data` directory.\n",
    "\n",
    "Set that directory in the `path` variable for ease of use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(DATA_PATH)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that we have four different folders inside:\n",
    "- `/water_bottle`\n",
    "- `/milk_bottle`\n",
    "- `/carton`\n",
    "- `/can`\n",
    "\n",
    "This is most common data format for multiclass image classification. Each folder title corresponds to the image label for the images contained inside:\n",
    "\n",
    "```\n",
    "/images\n",
    "+-- can (class 1)\n",
    "|   +-- image1.jpg\n",
    "|   +-- image2.jpg\n",
    "|   +-- ...\n",
    "+-- carton (class 2)\n",
    "|   +-- image31.jpg\n",
    "|   +-- image32.jpg\n",
    "|   +-- ...\n",
    "+-- ...\n",
    "```\n",
    "\n",
    "We have already set the data to this format structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Images\n",
    "\n",
    "In fast.ai, an `ImageDataBunch` can easily use multiple images (mini-batches) during training time. We create the `ImageDataBunch` by using [data_block apis](https://docs.fast.ai/data_block.html).\n",
    "\n",
    "For training and validation, we randomly split the data in an `8:2` ratio, holding 80% of the data for training and 20% for validation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (ImageList.from_folder(path) \n",
    "        .split_by_rand_pct(valid_pct=0.2, seed=10) \n",
    "        .label_from_folder() \n",
    "        .transform(size=IMAGE_SIZE) \n",
    "        .databunch(bs=BATCH_SIZE) \n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We examine some sample data using the `databunch` we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(rows=3, figsize=(15,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all available classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of classes: {data.c}')\n",
    "print(data.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the number of images in the training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.batch_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a standard analysis, we would split the data into a train/validate/test data sets. For this example, we do not use a test set but this could be added using the [add_test](https://docs.fast.ai/data_block.html#LabelLists.add_test) method. Note that in the fast.ai framework, test sets do not include labels as this should be the unknown data to be predicted. The validation data set is a test set that includes labels that can be used to measure the model performance on new observations not used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this image classifier, we use a **ResNet50** convolutional neural network (CNN) architecture. You can find more details about ResNet from [here](https://arxiv.org/abs/1512.03385).\n",
    "\n",
    "When training CNN, there are almost an infinite number of ways to construct the model architecture. We need to determine how many and what type of layers to include and how many nodes make up each layer. Other hyperparameters that control the training of those layers are also important and add to the overall complexity of neural net methods. With fast.ai, we use the `create_cnn` function to specify the model architecture and performance metric. We will use a transfer learning approach to reuse the CNN architecture and initialize the model parameters used to train on [ImageNet](http://www.image-net.org/).\n",
    "\n",
    "In this work, we use a custom callback `TrainMetricsRecorder` to track the model accuracy on the training set as we tune the model. This is for instruction only, as the standard approach in `fast.ai` [recorder class](https://docs.fast.ai/basic_train.html#Recorder) only supports tracking model accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(\n",
    "    data,\n",
    "    ARCHITECTURE,\n",
    "    metrics=[accuracy],\n",
    "    callback_fns=[partial(TrainMetricsRecorder, show_graph=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `unfreeze` method to allow us to retrain all the CNN layers with the <i>Fridge Objects</i> data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit` function trains the CNN using the parameters specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn.fit(EPOCHS, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can plot loss by using the default callback Recorder.\n",
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate the model\n",
    "\n",
    "To validate the model, calculate the model accuracy using the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, metric = learn.validate(learn.data.valid_dl, metrics=[accuracy])\n",
    "print(f'Accuracy on validation set: {100*float(metric):3.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ClassificationInterpretation` module is used to analyze the model classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "# Get prediction scores. We convert tensors to numpy array to plot them later.\n",
    "pred_scores = to_np(interp.probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see these details use the widget helper class `ResultsWidget`. The widget shows test images along with the ground truth label and model prediction score. With this tool, it's possible to see how the model predicts each image and debug the model if needed.\n",
    "\n",
    "<img src=\"https://cvbp.blob.core.windows.net/public/images/ic_widget.png\" width=\"600\"/>\n",
    "<center><i>Image Classification Result Widget</i></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w_results = ResultsWidget(\n",
    "    dataset=learn.data.valid_ds,\n",
    "    y_score=pred_scores,\n",
    "    y_label=[data.classes[x] for x in np.argmax(pred_scores, axis=1)]\n",
    ")\n",
    "display(w_results.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from accuracy, precision and recall are other metrics that are also important in classification settings. These are linked metrics that quantify how well the model classifies an image against how it fails. Since they are linked, there is a trade-off between optimizing for precision and optimizing for recall. They can be plotted against each other to graphically show how they are linked. \n",
    "\n",
    "In multiclass settings, we plot precision-recall and [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curves for each class. In this example, the dataset is not complex and the accuracy is close to 100%. In more difficult settings, these figures will be more interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True labels of the validation set. We convert to numpy array for plotting.\n",
    "true_labels = to_np(interp.y_true)\n",
    "plot_pr_roc_curves(true_labels, pred_scores, data.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix details the number of images on which the model succeeded or failed. For each class, the matrix lists correct classifications along the diagonal, and incorrect ones off-diagonal. This allows a detailed look on how the model confused the prediction of some classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating our results, we want to see where the model makes mistakes and if we can help it improve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "interp.plot_top_losses(9, figsize=(15,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is used by the notebook \"24_run_notebook_on_azureml.ipynb\" to log metrics when using papermill or scrapbook\n",
    "# to run this notebook. We can comment out this cell when we are running this notebook directly.\n",
    "\n",
    "training_losses = [x.numpy().ravel()[0] for x in learn.recorder.losses]\n",
    "training_accuracy = [x[0].numpy().ravel()[0] for x in learn.recorder.metrics]\n",
    "\n",
    "# pm.record may get deprecated and completely replaced by sb.glue: \n",
    "# https://github.com/nteract/scrapbook#papermills-deprecated-record-feature\n",
    "try:\n",
    "    sb.glue('training_loss', training_losses)\n",
    "    sb.glue('training_accuracy', training_accuracy)\n",
    "    sb.glue(\"Accuracy on validation set:\", 100*float(metric))\n",
    "except:\n",
    "    pm.record('training_loss', training_losses)\n",
    "    pm.record('training_accuracy', training_accuracy)\n",
    "    pm.record(\"Accuracy on validation set:\", 100*float(metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Using the concepts introduced in this notebook, you can bring your own dataset and train an image classifier to detect objects of interest for your specific setting. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cvbp)",
   "language": "python",
   "name": "cvbp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
