{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different Hyperparameters and Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll cover how to test different hyperparameters for a particular dataset and how to benchmark different parameters across a group of datasets using AzureML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to [11_exploring_hyperparameters.ipynb](https://github.com/microsoft/ComputerVision/blob/master/classification/notebooks/11_exploring_hyperparameters.ipynb), we will learn more about how different learning rates and different image sizes affect our model's accuracy when restricted to 10 epochs, and we want to build an AzureML experiment to test out these hyperparameters. \n",
    "\n",
    "We will be using a ResNet50 model to classify a set of images into 4 categories - 'can', 'carton', 'milk_bottle', 'water_bottle'. We will then conduct hyper-parameter tuning to find the best set of parameters for this model. For this,\n",
    "we present an overall process of utilizing AzureML, specifically [Hyperdrive](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive?view=azure-ml-py) component to run this tuning in parallel (and not successively).We demonstrate the following key steps:  \n",
    "* Configure AzureML Workspace\n",
    "* Create Remote Compute Target (GPU cluster)\n",
    "* Prepare Data\n",
    "* Prepare Training Script\n",
    "* Setup and Run Hyperdrive Experiment\n",
    "* Model Import, Re-train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "import azureml.data\n",
    "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal, choice\n",
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "import azureml.widgets as widgets\n",
    "\n",
    "from utils_cv.classification.data import Urls\n",
    "from utils_cv.common.data import unzip_url\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure edits to libraries are loaded and plotting is shown in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Config AzureML workspace\n",
    "Below we setup AzureML workspace and get all its details as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.setup()\n",
    "ws_details = ws.get_details()\n",
    "print('Name:\\t\\t{}\\nLocation:\\t{}'\n",
    "      .format(ws_details['name'],\n",
    "              ws_details['location']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Remote Target\n",
    "We create a GPU cluster as our remote compute target. If a cluster with the same name already exists in our workspace, the script will load it instead. We can see [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#compute-targets-for-training) to learn more about setting up a compute target on different locations.\n",
    "\n",
    "This notebook selects STANDARD_NC6 virtual machine (VM) and sets its priority as 'lowpriority' to reduce costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a name for our cluster\n",
    "cluster_name = \"gpu-cluster-nc6\"\n",
    "# Remote compute (cluster) configuration. If you want to reduce costs even more, set these to small.\n",
    "# For example, using Standard_DS1_v2 instead of using STANDARD_NC6\n",
    "VM_SIZE = 'STANDARD_NC6'\n",
    "VM_PRIORITY = 'lowpriority'\n",
    "\n",
    "# Cluster nodes\n",
    "MIN_NODES = 0\n",
    "MAX_NODES = 4\n",
    "\n",
    "try:\n",
    "    # Retrieve if a compute target with the same cluster_name already exists\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, we create a new one with the name provided\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=VM_SIZE,\n",
    "                                                           min_nodes=MIN_NODES,\n",
    "                                                           max_nodes=MAX_NODES)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# we can use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare data\n",
    "In this notebook, we'll use the Fridge Objects dataset, which is already stored in the correct format. We then upload our data to the AzureML workspace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, all the files under DATA will be uploaded to the data store\n",
    "DATA = unzip_url(Urls.fridge_objects_path, exist_ok=True)\n",
    "REPS = 3\n",
    "\n",
    "# Retrieving default datastore that got automatically created when we setup a workspace\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "# We now upload the data to the 'data' folder on the Azure portal\n",
    "ds.upload(\n",
    "    src_dir=os.path.dirname(DATA),\n",
    "    target_path='data',\n",
    "    overwrite=True, # with \"overwrite=True\", if this data already exists on the Azure blob storage, it will be overwritten\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here's where you can see the data in your portal: \n",
    "<img src=\"media/datastore.PNG\" width=\"800\" alt=\"Datastore screenshot for Hyperdrive notebook run\">\n",
    "\n",
    "### 4. Prepare training script\n",
    "\n",
    "Next step is to prepare scripts that AzureML Hyperdrive will use to train and evaluate models with selected hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a folder for the training script here\n",
    "script_folder = os.path.join(os.getcwd(), \"hyperparameter\")\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.externals import joblib\n",
    "import sys\n",
    "\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.vision.data import *\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "# Define parameters that we are going to use for training\n",
    "ARCHITECTURE  = models.resnet50\n",
    "EPOCHS = 10\n",
    "\n",
    "# Parse arguments passed by Hyperdrive\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "\n",
    "# Data path\n",
    "parser.add_argument('--data-folder', type=str, dest='DATA_DIR', help=\"Datastore path\")\n",
    "parser.add_argument('--im_size', type=int, dest='IM_SIZE')\n",
    "parser.add_argument('--learning_rate', type=float, dest='LEARNING_RATE')\n",
    "\n",
    "args = parser.parse_args()\n",
    "params = vars(args)\n",
    "\n",
    "if params['IM_SIZE'] is None:\n",
    "     raise ValueError(\"Image Size empty\")\n",
    "        \n",
    "if params['LEARNING_RATE'] is None:\n",
    "    raise ValueError(\"Learning Rate empty\")\n",
    "\n",
    "if params['DATA_DIR'] is None:\n",
    "    raise ValueError(\"Data folder empty\")\n",
    "    \n",
    "\n",
    "path = params['DATA_DIR'] + '/data/fridgeObjects'\n",
    "\n",
    "# Getting training and validation data and training the CNN as done in 01_training_introduction.ipynb\n",
    "data = (ImageList.from_folder(path)\n",
    "        .split_by_rand_pct(valid_pct=0.2, seed=10)\n",
    "        .label_from_folder() \n",
    "        .transform(size=params['IM_SIZE']) \n",
    "        .databunch(bs=16) \n",
    "        .normalize(imagenet_stats))\n",
    "\n",
    "learn = cnn_learner(\n",
    "    data,\n",
    "    ARCHITECTURE,\n",
    "    metrics=[accuracy]\n",
    ")\n",
    "\n",
    "learn.unfreeze()\n",
    "learn.fit(EPOCHS, params['LEARNING_RATE'])\n",
    "\n",
    "training_losses = [x.numpy().ravel()[0] for x in learn.recorder.losses]\n",
    "accuracy = [x[0].numpy().ravel()[0] for x in learn.recorder.metrics][-1]\n",
    "\n",
    "#run.log_list('training_loss', training_losses)\n",
    "#run.log_list('validation_loss', learn.recorder.val_losses)\n",
    "#run.log_list('error_rate', error_rate)\n",
    "run.log('data_dir',params['DATA_DIR'])\n",
    "run.log('im_size', params['IM_SIZE'])\n",
    "run.log('learning_rate', params['LEARNING_RATE'])\n",
    "run.log('accuracy', float(accuracy))  # Logging our primary metric 'accuracy'\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "output_folder = os.path.join(current_directory, 'outputs')\n",
    "MODEL_NAME = 'im_classif_resnet50'  # Name we will give our model both locally and on Azure\n",
    "PICKLED_MODEL_NAME = MODEL_NAME + '.pkl'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "learn.export(os.path.join(output_folder, PICKLED_MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Setup and run Hyperdrive experiment\n",
    "\n",
    "Next step is to prepare scripts that AzureML Hyperdrive will use to train and evaluate models with selected hyperparameters. To run the model notebook from the Hyperdrive Run, all we need is to prepare an entry script which parses the hyperparameter arguments, passes them to the notebook, and records the results of the notebook to AzureML Run logs. \n",
    "\n",
    "#### 5.1 Create Experiment  \n",
    "Experiment is the main entry point into experimenting with AzureML. To create new Experiment or get the existing one, we pass our experimentation name 'hyperparameter-tuning'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'hyperparameter-tuning'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Define search space\n",
    "\n",
    "Now we define the search space of hyperparameters. For example, if you want to test different batch sizes of {64, 128, 256}, you can use azureml.train.hyperdrive.choice(64, 128, 256). To search from a continuous space, use uniform(start, end). For more options, see [Hyperdrive parameter expressions](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.parameter_expressions?view=azure-ml-py).\n",
    "\n",
    "In this notebook we use the ResNet50 architecture, and fix the number of epochs to 10.\n",
    "In the search space, we set different learning rates and image sizes. Details about the hyperparameters can be found in [11_exploring_hyperparameters.ipynb notebook](https://github.com/microsoft/ComputerVision/blob/master/classification/notebooks/11_exploring_hyperparameters.ipynb).\n",
    "\n",
    "Hyperdrive provides three different parameter sampling methods: 'RandomParameterSampling', 'GridParameterSampling', and 'BayesianParameterSampling'. Details about each method can be found [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters). Here, we use the 'RandomParameterSampling'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_SIZES = [299, 499]\n",
    "LEARNING_RATES = [1e-3, 1e-4, 1e-5]\n",
    "\n",
    "# Hyperparameter search space\n",
    "param_sampling = RandomParameterSampling( {\n",
    "        '--learning_rate': choice(LEARNING_RATES),\n",
    "        '--im_size': choice(IM_SIZES)\n",
    "    }\n",
    ")\n",
    "\n",
    "primary_metric_name = 'accuracy'\n",
    "primary_metric_goal = PrimaryMetricGoal.MAXIMIZE\n",
    "max_total_runs=50\n",
    "max_concurrent_runs=4\n",
    "\n",
    "early_termination_policy = BanditPolicy(slack_factor=0.15, evaluation_interval=1, delay_evaluation=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>AzureML Estimator</b> is the building block for training. An Estimator encapsulates the training code and parameters, the compute resources and runtime environment for a particular training scenario.\n",
    "We create one for our experimentation with the dependencies our model requires as follows:\n",
    "\n",
    "```python\n",
    "pip_packages=['fastai']\n",
    "conda_packages=['scikit-learn']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    '--data-folder': ds.as_mount()\n",
    "}\n",
    "\n",
    "est = Estimator(source_directory=script_folder,\n",
    "                script_params=script_params,\n",
    "                compute_target=compute_target,\n",
    "                entry_script='train.py',\n",
    "                pip_packages=['fastai'],\n",
    "                conda_packages=['scikit-learn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a HyperDriveConfig object which  includes information about parameter space sampling, termination policy, primary metric, estimator and the compute target to execute the experiment runs on. We feed the following parameters to it:\n",
    "\n",
    "- our estimator object that we created in the above cell\n",
    "- hyperparameter sampling method, in this case it is Random Parameter Sampling\n",
    "- early termination policy, in this case we use [Bandit Policy](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters#bandit-policy)\n",
    "- primary metric name reported by our runs, in this case it is accuracy \n",
    "- the goal, which determines whether the primary metric has to be maximized/minimized, in this case it is to maximize our accuracy \n",
    "- number of total child-runs, in this case it is 4\n",
    "\n",
    "The bigger the search space, the more child-runs get triggered for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run_config = HyperDriveConfig(estimator=est,\n",
    "                                         hyperparameter_sampling=param_sampling,\n",
    "                                         policy=early_termination_policy,\n",
    "                                         primary_metric_name=primary_metric_name,\n",
    "                                         primary_metric_goal=primary_metric_goal,\n",
    "                                         max_total_runs=max_total_runs,\n",
    "                                         max_concurrent_runs= max_concurrent_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we submit the Run to our experiment. \n",
    "hyperdrive_run = exp.submit(config=hyperdrive_run_config)\n",
    "# We can see the experiment progress from this notebook by using \n",
    "widgets.RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can check from the Azure portal with the url link we get by running \n",
    "```python \n",
    "hyperdrive_run.get_portal_url().```\n",
    "\n",
    "To load an existing Hyperdrive Run instead of start new one, we can use \n",
    "```python\n",
    "hyperdrive_run = azureml.train.hyperdrive.HyperDriveRun(exp, <your-run-id>, hyperdrive_run_config=hyperdrive_run_config)\n",
    "```\n",
    "We also can cancel the Run with \n",
    "```python \n",
    "hyperdrive_run_config.cancel().\n",
    "```\n",
    "\n",
    "Once all the child-runs are finished, we can get the best run and the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best run and print out metrics\n",
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "parameter_values = best_run.get_details()['runDefinition']['arguments']\n",
    "best_parameters = dict(zip(parameter_values[::2], parameter_values[1::2]))\n",
    "\n",
    "print(f\"* Best Run Id:{best_run.id}\")\n",
    "print(best_run)\n",
    "print(\"\\n* Best hyperparameters:\")\n",
    "print(best_parameters)\n",
    "print(f\"Accuracy = {best_run_metrics['accuracy']}\")\n",
    "#print(\"Learning Rate =\", best_run_metrics['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Test the model\n",
    "\n",
    "We can download the best model from the outputs/ folder and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "output_folder = os.path.join(current_directory, 'outputs')\n",
    "MODEL_NAME = 'im_classif_resnet50'\n",
    "PICKLED_MODEL_NAME = MODEL_NAME + '.pkl'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for f in best_run.get_file_names():\n",
    "    if f.startswith('outputs/im_classif_resnet50'):\n",
    "        print(\"Downloading {}..\".format(f))\n",
    "        best_run.download_file(name=f, output_file_path=output_folder)\n",
    "saved_model = load_learner(path = output_folder, file=PICKLED_MODEL_NAME)\n",
    "print(saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the retrieved best model to get predictions on unseen images as done in [02_training_accuracy_vs_speed.ipynb](https://github.com/microsoft/ComputerVision/blob/master/classification/notebooks/03_training_accuracy_vs_speed.ipynb) notebook using\n",
    "```python\n",
    "saved_model.predict(image)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
