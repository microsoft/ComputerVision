{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models for Accuracy VS Speed\n",
    "\n",
    "The goal of this notebook is to understand how to train a model with different parameters to achieve either a highly accurate but slow inference, or a model with fast inference but lower accuracy.\n",
    "\n",
    "As practitioners of computer vision, we want to be able to control what to optimize when building our models. Unless you are building a model for a Kaggle competition, it is unlikely that you can build your model with only accuracy in mind. \n",
    "\n",
    "For example, in IoT settings the inferencing device has limited computational capabilities. We then need to design our models to have a small memory footprint. In contrast, medical situations often require the highest possible accuracy because the cost of mis-classification could impact the well-being of a patient. In this scenario, the accuracy of the model can not be compromised. \n",
    "\n",
    "We have conducted various experiments on diverse datasets to find parameters which work well in a wide variety of settings balancing high accuracy or fast inference. In this notebook, we provide these parameters so that your initial models can be trained without any parameter tuning. For most datasets, these parameters are close to optimal. In the second part of the notebook, we provide guidelines  to how fine tuning these parameters how they impact the model.\n",
    "\n",
    "We recommended first training your model with the default parameters, evaluate the results, and then fine tuning parameters to achieve better results as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "* [Training a High Accuracy or a Fast Inference Speed Classifier ](#model)\n",
    "  * [Choosing between two types of models](#choosing)\n",
    "  * [Pre-processing](#preprocessing)\n",
    "  * [Training](#training)\n",
    "  * [Evaluation](#evaluation)\n",
    "* [Fine tuning our models](#finetuning)\n",
    "  * [DNN Architecture](#dnn)\n",
    "  * [Key Parameters](#key-parameters)\n",
    "  * [Other Parameters](#other-parameters)\n",
    "  * [Testing Parameters](#testing-parameters)\n",
    "* [Appendix](#appendix)\n",
    "  * [Learning Rate](#appendix-learning-rate)\n",
    "  * [Image Size](#appendix-imsize)\n",
    "  * [How we got good parameters](#appendix-good-parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a High Accuracy or a Fast Inference Speed Classifier <a name=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first verify our fastai version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.48'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fastai\n",
    "fastai.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure edits to libraries are loaded and plotting is shown in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `fastai`. For now, we'll import all (`import *`) so that we can easily use different utilies provided by the fastai library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "from pathlib import Path\n",
    "from utils_cv.classification.data import Urls\n",
    "from utils_cv.common.data import unzip_url\n",
    "from fastai.vision import *\n",
    "from fastai.metrics import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've set up our notebook, lets set the hyperparameters based on which model type was selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing between two types of models <a name=\"choosing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most scenarios, computer vision practitioners want to create a high accuracy model, a fast-inference model or a small size model. Set your `MODEL_TYPE` variable to one of the following: `\"high_accuracy\"`, `\"fast_inference\"`, or `\"small_size\"`.\n",
    "\n",
    "we will again use the `FridgeObjects` dataset from [previous notebook](01_training_introduction.ipynb). You can replace the `DATA_PATH` variable with your own data.\n",
    "\n",
    "When choosing the batch size, remember that even mid-level GPUs run out of memory when training a deeper resnet models with larger image resolutions. If you get an _out of memory_ error, try reducing the batch size by a factor of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Choose between \"high_accuracy\", \"fast_inference\", or \"small_size\"\n",
    "MODEL_TYPE = \"fast_inference\"\n",
    "\n",
    "# Path to your data\n",
    "DATA_PATH = unzip_url(Urls.fridge_objects_path, exist_ok=True)\n",
    "\n",
    "# Epochs to train for\n",
    "EPOCHS_HEAD = 4\n",
    "EPOCHS_BODY = 12\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 16 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that only `MODEL_TYPE` is correctly set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert MODEL_TYPE in [\"high_accuracy\", \"fast_inference\", \"small_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters based on your selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TYPE == \"high_accuracy\":\n",
    "    ARCHITECTURE = models.resnet50\n",
    "    IM_SIZE = 500 \n",
    "    \n",
    "if MODEL_TYPE == \"fast_inference\":\n",
    "    ARCHITECTURE = models.resnet18\n",
    "    IM_SIZE = 300 \n",
    "\n",
    "if MODEL_TYPE == \"small_size\":\n",
    "    ARCHITECTURE = models.squeezenet1_1\n",
    "    IM_SIZE = 300 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing <a name=\"preprocessing\"></a>\n",
    "\n",
    "JPEG decoding represents a bottleneck on systems with powerful GPUs and can slow training significantly, often by a factor of 2-3x or more. We recommend creating a down-sized copy of the dataset if training takes too long, or if you require multiple training runs to evaluate different parameters. \n",
    "\n",
    "The following function will automate image downsizing.\n",
    "```python\n",
    "from utils_cv.classification.data import downsize_imagelist\n",
    "\n",
    "downsize_imagelist(im_list = ImageList.from_folder(Path(DATA_PATH)),\n",
    "                   out_dir = \"downsized_images\", \n",
    "                   max_dim = IM_SIZE)\n",
    "```\n",
    "\n",
    "Once complete, update the `DATA_PATH` variable (to `out_dir`) so that this notebook uses these resized images. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training <a name=\"training\"></a>\n",
    "\n",
    "We'll now re-apply the same steps we did in the [training introduction](01_training_introduction.ipynb) notebook here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (ImageList.from_folder(Path(DATA_PATH)) \n",
    "        .split_by_rand_pct(valid_pct=0.2, seed=10) \n",
    "        .label_from_folder() \n",
    "        .transform(tfms=get_transforms(), size=IM_SIZE) \n",
    "        .databunch(bs=16) \n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(data, ARCHITECTURE, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the last layer for a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.710471</td>\n",
       "      <td>1.451503</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.804611</td>\n",
       "      <td>1.420726</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.704576</td>\n",
       "      <td>1.411111</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.665398</td>\n",
       "      <td>1.410190</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>00:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(EPOCHS_HEAD, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfreeze the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune the network for the remaining epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:19 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.623785</td>\n",
       "      <td>1.409176</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.457880</td>\n",
       "      <td>1.230384</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.346284</td>\n",
       "      <td>0.825346</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.222301</td>\n",
       "      <td>0.543954</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.059379</td>\n",
       "      <td>0.393587</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.920777</td>\n",
       "      <td>0.315344</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.807599</td>\n",
       "      <td>0.258829</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.712808</td>\n",
       "      <td>0.239849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.634236</td>\n",
       "      <td>0.231437</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.570075</td>\n",
       "      <td>0.237903</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.511892</td>\n",
       "      <td>0.240423</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.470356</td>\n",
       "      <td>0.234572</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(EPOCHS_BODY, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation <a name=\"evaluation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we test our model on the following characteristics:\n",
    "- accuracy\n",
    "- inference speed\n",
    "- parameter export size / memory footprint required\n",
    "\n",
    "\n",
    "Refer back to the [training introduction](01_training_introduction.ipynb) to learn about other ways to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy \n",
    "To keep things simple, we just a look at the final accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set: 1.0\n"
     ]
    }
   ],
   "source": [
    "_, metric = learn.validate(learn.data.valid_dl, metrics=[accuracy])\n",
    "print(f'Accuracy on validation set: {float(metric)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference speed\n",
    "\n",
    "Use the model to inference and time how long it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = open_image(f\"{(Path(DATA_PATH)/learn.data.classes[0]).ls()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.6 ms ± 375 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "learn.predict(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory footprint\n",
    "\n",
    "Export our model and inspect the size of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(f\"{MODEL_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'fast_inference' is 44.77MB.\n"
     ]
    }
   ],
   "source": [
    "size_in_mb = os.path.getsize(Path(DATA_PATH)/MODEL_TYPE) / (1024*1024.)\n",
    "print(f\"'{MODEL_TYPE}' is {round(size_in_mb, 2)}MB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning parameters <a name=\"finetuning\"></a>\n",
    "\n",
    "If you use the parameters provided with the defaults Fastai parameters, you can get good results across a wide variety of datasets. However, as in most machine learning projects, getting the best possible results for a new dataset often requires tuning the parameters further. The following section provides guidelines on optimizing for accuracy, inference speed, or model size on a given dataset. We'll go through the parameters that will make the largest impact on your model as well as the parameters that may not be worth modifying.\n",
    "\n",
    "Generally speaking, models for image classification come with a trade-off between training time versus model accuracy. The four parameters that most affect this trade-off are the DNN architecture, image resolution, learning rate, and number of epochs. DNN architecture and image resolution will additionally affect the model's inference time and memory footprint. As a rule of thumb, deeper networks with high image resolution will achieve higher accuracy at the cost of large model sizes and low training and inference speeds. Shallow networks with low image resolution will result in models with fast inference speed, fast training speeds and low model sizes at the cost of the model accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Architectures <a name=\"dnn\"></a>\n",
    "\n",
    "When choosing at an architecture, we want to make sure it fits our requirements for accuracy, memory footprint, inference speed and training speeds. Some DNNs have hundreds of layers and end up with a large memory footprint and millions of parameters to tune, while others are compact and small enough to fit onto memory limited edge devices. \n",
    "\n",
    "Lets take a __squeezenet1_1__ model, a __resnet18__ model and __resnet50__ model and compare these using an experiment over diverse set of 6 datasets. (More about the datasets in the appendix below)\n",
    "\n",
    "![architecture_comparisons](media/architecture_comparisons.png)\n",
    "\n",
    "As you can see from the graph, there is a clear trade-off when deciding between the models. \n",
    "\n",
    "In terms of accuracy, __resnet50__ out-performs the rest, but it also suffers from having the highest memory footprint, and the longest training and inference times. Alternatively, __squeezenet1_1__ performs the worst in terms of accuracy, but has the smallest memory footprint.\n",
    "\n",
    "Generally speaking, given enough data, the deeper DNN and the higher the image resolution, the higher the accuracy you'll be able to achieve with your model.\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>See the code to generate the graphs</summary>\n",
    "<p>\n",
    "\n",
    "### Code snippet to generate graphs in this cell\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from utils_ic.parameter_sweeper import add_value_labels\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"accuracy\": [.9472, .9190, .8251],\n",
    "    \"training_duration\": [385.3, 280.5, 272.5],\n",
    "    \"inference_duration\": [34.2, 27.8, 27.6],\n",
    "    \"memory\": [99, 45, 4.9],\n",
    "    \"model\": ['resnet50', 'resnet18', 'squeezenet1_1'],\n",
    "}).set_index(\"model\")\n",
    "\n",
    "ax1, ax2, ax3, ax4 = df.plot.bar(\n",
    "    rot=90, subplots=True, legend=False, figsize=(8,10)\n",
    ")\n",
    "\n",
    "for ax in [ax1, ax2, ax3, ax4]:\n",
    "    for i in [0, 1, 2]:\n",
    "        if i==0: ax.get_children()[i].set_color('r') \n",
    "        if i==1: ax.get_children()[i].set_color('g') \n",
    "        if i==2: ax.get_children()[i].set_color('b') \n",
    "\n",
    "ax1.set_title(\"Accuracy (%)\")\n",
    "ax2.set_title(\"Training Duration (seconds)\")\n",
    "ax3.set_title(\"Inference Time (seconds)\")\n",
    "ax4.set_title(\"Memory Footprint (mb)\")\n",
    "\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax2.set_ylabel(\"seconds\")\n",
    "ax3.set_ylabel(\"seconds\")\n",
    "ax4.set_ylabel(\"mb\")\n",
    "\n",
    "ax1.set_ylim(top=df[\"accuracy\"].max() * 1.3)\n",
    "ax2.set_ylim(top=df[\"training_duration\"].max() * 1.3)\n",
    "ax3.set_ylim(top=df[\"inference_duration\"].max() * 1.3)\n",
    "ax4.set_ylim(top=df[\"memory\"].max() * 1.3)\n",
    "\n",
    "add_value_labels(ax1, percentage=True)\n",
    "add_value_labels(ax2)\n",
    "add_value_labels(ax3)\n",
    "add_value_labels(ax4)\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Parameters <a name=\"key-parameters\"></a>\n",
    "This section examines some of the key parameters when training a deep learning model for image classification. The table below shows default parameters we recommend using.\n",
    "\n",
    "| Parameter | Default Value |\n",
    "| --- | --- |\n",
    "| Learning Rate | 1e-4 |\n",
    "| Epochs | 15 |\n",
    "| Batch Size | 16 |\n",
    "| Image Size | 300 X 300 |\n",
    "\n",
    "__Learning rate__ \n",
    "\n",
    "Learning rate step size is used when optimizing your model with gradient descent and tends to be one of the most important parameters to set when training your model. If your learning rate is set too low, training will progress very slowly since we're only making tiny updates to the weights in your network. However, if your learning rate is too high, it can cause undesirable divergent behavior in your loss function. Generally speaking, choosing a learning rate of 1e-4 was shown to work pretty well for most datasets. If you want to reduce training time (by training for fewer epochs), you can try setting the learning rate to 5e-3, but if you notice a spike in the training or validation loss, you may want to try reducing your learning rate.\n",
    "\n",
    "You can learn more about learning rate in the [appendix below](#appendix-learning-rate).\n",
    "\n",
    "__Epochs__\n",
    "\n",
    "When it comes to choosing the number of epochs, a common question is - _Won't too many epochs will cause overfitting_? It turns out that the accuracy on the test set typically does not get worse, even if training for too many epochs. Unless your are working with small datasets, using around 15 epochs tends to work pretty well in most cases.\n",
    "\n",
    "\n",
    "__Batch Size__\n",
    "\n",
    "Batch size is the number of training samples you use in order to make one update to the model parameters. A batch size of 16 or 32 works well for most cases. The higher the batch size, the faster training will be, but at the expense of an increased DNN memory consumption. Depending on your dataset and the GPU you have, you can start with a batch size of 32, and move down to 16 if your GPU doesn't have enough memory. After a certain increase in batch size, improvments to training speed become marginal, hence we found 16 (or 32) to be a good trade-off between training speed and memory consumption.If you reduce the batch size, you may also have to reduce the learning rate.\n",
    "\n",
    "__Image size__ \n",
    "\n",
    "The default image size is __300 X 300__ pixels. Using higher image resolution of, for example, __500 X 500__ or even higher, can improve the accuracy of the model but at the cost of longer training and inference times.\n",
    "\n",
    "You can learn more about the impact of image resolution in the [appendix below](#appendix-imsize).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Parameters <a name=\"other-parameters\"></a>\n",
    "\n",
    "In this section, we examine some other common hyperparameters used to tune DNNs. Note that the exact value of these parameters does not have a large impact on model performance, training/inference speed, or memory footprint. \n",
    "\n",
    "| Parameter | Good Default Value |\n",
    "| --- | --- |\n",
    "| Dropout | 0.5 or (0.5 on the final layer and 0.25 on all previous layers) |\n",
    "| Weight Decay | 0.01 |\n",
    "| Momentum | 0.9 or (min=0.85 and max=0.95 when using cyclical momentum) |\n",
    "\n",
    "__Dropout__\n",
    "\n",
    "Dropout is used to discard activations at random when training your model. It is a way to keep the model from over-fitting on the training data. In Fastai, dropout is set to 0.5 by default on the final layer, and 0.25 on all other layer. Unless there is clear evidence of over-fitting, this drop out tends to work well.\n",
    "\n",
    "__Weight decay (L2 regularization)__\n",
    "\n",
    "Weight decay is a regularization term applied to help minimize the network loss function. We can think of it as a penalty applied to the weights after an update toprevent the weights from growing very large. In Fastai, the default weight decay is 0.1, which we find to be almost always acceptable. \n",
    "\n",
    "__Momentum__\n",
    "\n",
    "Momentum is a way to accelerate convergence when training a model. Momentum uses a weighted average of the most recent updates applied to the current update. Fastai implements cyclical momentum when calling `fit_one_cycle()`, so the momentum will fluctuate over the course of the training cycle. We control this by setting a min and max value for momentum. \n",
    "\n",
    "When using `fit_one_cycle()`, the default value of max=0.95 and min=0.85 is known to work well. If using `fit()`, the default value of 0.9 has been shown to work well. These defaults provided by Fastai represent a good trade-off between training speed and the ability of the model to converge to a good solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Parameters <a name=\"testing-parameters\"></a>\n",
    "The `ParameterSweeper` module can be used to search over the parameter space to locate the \"best\" value for that parameter. See the [exploring hyperparameters notebook](./11_exploring_hyperparameters.ipynb) for more information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix <a name=\"appendix\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate <a name=\"appendix-learning-rate\"></a>\n",
    "\n",
    "Setting a low learning rate requires training for many epochs to reach convergence. However, each additional epoch required directly increases the model training time. To efficiently build a model, it helps to set the learning rate is in the correct range. To demonstrate this, we've tested various learning rates on 6 different datasets, training the full network for 3 or 15 epochs.\n",
    "\n",
    "![lr_comparisons](media/lr_comparisons.png)\n",
    "\n",
    "<details><summary><em>Understanding the diagram</em></summary>\n",
    "<p>\n",
    "    \n",
    "> The figure on the left shows results of different learning rates on different datasets at 15 epochs. We see that a learning rate of 1e-4 results in the the best overall accuracy, but this may not be the case for every dataset. Notice there is a pretty significant variance between the datasets and it may a learning rate of 1-e3 may work better for some datasets. \n",
    "In the figure on the right, both 1e-4 and 1e-3 seem to work well. At 15 epochs, the results of 1e-4 are only slightly better than that of 1e-3. However, at 3 epochs, a learning rate of 1e-3 out performs the learning rate at 1e-4. This makes sense since we're limiting the training to only 3 epochs and the model that updates weights more quickly will perform better. This result indicates higher learning rates (such as 1e-3) may help minimize the training time, and lower learning rates (such as 1e-4) may be better if training time is not constrained. \n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "In both figures, we can see that a learning rate of 1e-3 and 1e-4 tends to work best across all cases. We observe that training with 3 epochs gives lower accuracy compared to 15 epochs. In some datasets, higher learning rates prevent the DNN from converging. \n",
    "\n",
    "Fastai has implemented [one cycle policy with cyclical momentum](https://arxiv.org/abs/1803.09820) which adaptively optimizes the learning rate. This function takes a maximum learning rate value as an argument to manually help the method avoid this convergence problem. Replace the `fit()` method with `fit_one_cycle()` to use this capability.\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>See the code to generate the graphs</summary>\n",
    "<p>\n",
    "\n",
    "### Code snippet to generate graphs in this cell\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df_dataset_comp = pd.DataFrame({\n",
    "    \"fashionTexture\": [0.8749, 0.8481, 0.2491, 0.670318, 0.1643],\n",
    "    \"flickrLogos32Subset\": [0.9069, 0.9064, 0.2179, 0.7175, 0.1073],\n",
    "    \"food101Subset\": [0.9294, 0.9127, 0.6891, 0.9090, 0.555827],\n",
    "    \"fridgeObjects\": [0.9591, 0.9727, 0.272727, 0.6136, 0.181818],\n",
    "    \"lettuce\": [0.8992, 0.9104, 0.632, 0.8192, 0.5120],\n",
    "    \"recycle_v3\": [0.9527, 0.9581, 0.766, 0.8591, 0.2876],\n",
    "    \"learning_rate\": [0.000100, 0.001000, 0.010000, 0.000010, 0.000001]\n",
    "}).set_index(\"learning_rate\")\n",
    "\n",
    "df_epoch_comp = pd.DataFrame({\n",
    "    \"3_epochs\": [0.823808, 0.846394, 0.393808, 0.455115, 0.229120],\n",
    "    \"15_epochs\": [0.920367, 0.918067, 0.471138, 0.764786, 0.301474],\n",
    "    \"learning_rate\": [0.000100, 0.001000, 0.010000, 0.000010, 0.000001]\n",
    "}).set_index(\"learning_rate\")\n",
    "\n",
    "plt.figure(1)\n",
    "ax1 = plt.subplot(121)\n",
    "ax2 = plt.subplot(122)\n",
    "\n",
    "vals = ax2.get_yticks()\n",
    "\n",
    "df_dataset_comp.sort_index().plot(kind='bar', rot=0, figsize=(15, 6), ax=ax1)\n",
    "vals = ax1.get_yticks()\n",
    "ax1.set_yticklabels(['{:,.2%}'.format(x) for x in vals])\n",
    "ax1.set_ylim(0,1)\n",
    "ax1.set_ylabel(\"Accuracy (%)\")\n",
    "ax1.set_title(\"Accuracy of Learning Rates by Datasets @ 15 Epochs\")\n",
    "ax1.legend(loc=2)\n",
    "\n",
    "df_epoch_comp.sort_index().plot(kind='bar', rot=0, figsize=(15, 6), ax=ax2)\n",
    "ax2.set_yticklabels(['{:,.2%}'.format(x) for x in vals])\n",
    "ax2.set_ylim(0,1)\n",
    "ax2.set_title(\"Accuracy of Learning Rates by Epochs\")\n",
    "ax2.legend(loc=2)\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Resolution <a name=\"appendix-imsize\"></a>\n",
    "\n",
    "A model's input image resolution also tends to affect its accuracy. Usually, convolutional neural networks are able to take advantage of higher resolution images, especially if the object-of-interest is small in the overall image.\n",
    "\n",
    "But how does it impact some of the other aspects of the model? \n",
    "\n",
    "We find that image size doesn't affect the model's memory footprint, but it has a huge effect on GPU memory. Image size also impacts training and inference speeds.\n",
    "\n",
    "![imsize_comparisons](media/imsize_comparisons.png)\n",
    "\n",
    "From the results, we can see that an increase in image resolution from __300 X 300__ to __500 X 500__ will increase the performance marginally at the cost of a longer training duration and slower inference speed.\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>See the code to generate the graphs</summary>\n",
    "<p>\n",
    "\n",
    "### Code snippet to generate graphs in this cell\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from utils_ic.parameter_sweeper import add_value_labels\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"accuracy\": [.9472, .9394, .9190, .9164, .8366, .8251],\n",
    "    \"training_duration\": [385.3, 218.8, 280.5, 184.9, 272.5, 182.3],\n",
    "    \"inference_duration\": [34.2, 23.2, 27.8, 17.8, 27.6, 17.3],\n",
    "    \"model\": ['resnet50 X 499', 'resnet50 X 299', 'resnet18 X 499', 'resnet18 X 299', 'squeezenet1_1 X 499', 'squeezenet1_1 X 299'],\n",
    "}).set_index(\"model\"); df\n",
    "\n",
    "ax1, ax2, ax3 = df.plot.bar(\n",
    "    rot=90, subplots=True, legend=False, figsize=(12, 12)\n",
    ")\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if i < len(df)/3:\n",
    "        ax1.get_children()[i].set_color('r')\n",
    "        ax2.get_children()[i].set_color('r')\n",
    "        ax3.get_children()[i].set_color('r')\n",
    "    if i >= len(df)/3 and i < 2*len(df)/3:\n",
    "        ax1.get_children()[i].set_color('g')\n",
    "        ax2.get_children()[i].set_color('g')\n",
    "        ax3.get_children()[i].set_color('g')  \n",
    "    if i >= 2*len(df)/3:\n",
    "        ax1.get_children()[i].set_color('b')\n",
    "        ax2.get_children()[i].set_color('b')\n",
    "        ax3.get_children()[i].set_color('b') \n",
    "\n",
    "ax1.set_title(\"Accuracy (%)\")\n",
    "ax2.set_title(\"Training Duration (seconds)\")\n",
    "ax3.set_title(\"Inference Speed (seconds)\")  \n",
    "\n",
    "ax1.set_ylabel(\"%\")\n",
    "ax2.set_ylabel(\"seconds\")\n",
    "ax3.set_ylabel(\"seconds\")\n",
    "\n",
    "ax1.set_ylim(top=df[\"accuracy\"].max() * 1.2)\n",
    "ax2.set_ylim(top=df[\"training_duration\"].max() * 1.2)\n",
    "ax3.set_ylim(top=df[\"inference_duration\"].max() * 1.2)\n",
    "\n",
    "add_value_labels(ax1, percentage=True)\n",
    "add_value_labels(ax2)\n",
    "add_value_labels(ax3)\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How we found good default parameters <a name=\"#appendix-good-parameters\"></a>\n",
    "\n",
    "To explore the charactistics of a model, we conducted various experiments to explore the impact of different hyperparameters on a model's _accuracy_, _training duration_, _inference speed_, and _memory footprint_. \n",
    "\n",
    "### Datasets <a name=\"datasets\"></a>\n",
    "\n",
    "For our experiments, we relied on a set of six different classification datasets. When selecting these datasets, we wanted to have a variety of image types with different amounts of data and number of classes. \n",
    "\n",
    "| Dataset Name | Number of Images | Number of Classes | \n",
    "| --- | --- | --- |\n",
    "| food101Subset | 5000 | 5 | \n",
    "| flickrLogos32Subset | 2740 | 33 | \n",
    "| fashionTexture | 1716 | 11 | \n",
    "| recycle_v3 |  564 | 11 | \n",
    "| lettuce | 380 | 2 |\n",
    "| fridgeObjects | 134 | 4 | \n",
    "\n",
    "### Model Characteristics <a name=\"model-characteristics\"></a>\n",
    "\n",
    "In our experiment, we look at these characteristics to evaluate the impact of various paramters. Here is how we calculated each of the following metrics:\n",
    "\n",
    "- __Accuracy__ metric is averaged over 5 runs for our six different datasets. \n",
    "\n",
    "\n",
    "- __Training Duration__ metric is the average duration over 5 runs for our six different datasets.\n",
    "\n",
    "\n",
    "- __Inference Speed__ is the time it takes the model to run 1000 predictions.\n",
    "\n",
    "\n",
    "- __Memory Footprint__ is the size of the model parameter pickle file output from the `learn.export(...)` method. \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cvbp)",
   "language": "python",
   "name": "cvbp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
