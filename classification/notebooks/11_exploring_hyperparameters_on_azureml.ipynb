{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different Hyperparameters and Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll cover how to test different hyperparameters for a particular dataset and how to benchmark different parameters across a group of datasets.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "* [Testing parameter](#hyperparam)\n",
    "  * [Using Python](#python)\n",
    "  * [Using the CLI](#cli)\n",
    "  * [Visualizing the results](#visualize)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing hyperparameters  <a name=\"hyperparam\"></a>\n",
    "\n",
    "Lets say we want to learn more about __how different learning rates and different image sizes affect our model's accuracy when restricted to 10 epochs__, and we want to build an experiment to test out these hyperparameters. We also want to try these parameters out on two different variations of the dataset - one where the images are kept raw (maybe there is a watermark on the image) and one where the images have been altered (the same dataset where there was some attempt to remove the watermark).\n",
    "\n",
    "In this notebook, we'll walk through how we use the Parameter Sweeper module with the following:\n",
    "\n",
    "- use python to perform this experiment\n",
    "- use the CLI to perform this experiment\n",
    "- evalute the results using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out fastai version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "from fastai.vision import *\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from utils_cv.classification.data import Urls\n",
    "from utils_cv.common.data import unzip_url\n",
    "from utils_cv.classification.parameter_sweeper import *\n",
    "from utils_cv.classification.model import TrainMetricsRecorder\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.train.estimator import Estimator\n",
    "import azureml.data\n",
    "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveConfig, uniform, PrimaryMetricGoal, choice\n",
    "import azureml.widgets as widgets\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure edits to libraries are loaded and plotting is shown in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.setup()\n",
    "ws_details = ws.get_details()\n",
    "print('Name:\\t\\t{}\\nLocation:\\t{}'\n",
    "      .format(ws_details['name'],\n",
    "              ws_details['location']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a name for your cluster\n",
    "cluster_name = \"gpu-cluster-nc24\"\n",
    "# Remote compute (cluster) configuration. If you want to save the cost more, set these to small.\n",
    "VM_SIZE = 'STANDARD_NC24'\n",
    "VM_PRIORITY = 'lowpriority'\n",
    "\n",
    "# Cluster nodes\n",
    "MIN_NODES = 0\n",
    "MAX_NODES = 4\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=VM_SIZE,\n",
    "                                                           min_nodes=MIN_NODES,\n",
    "                                                           max_nodes=MAX_NODES)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, all the files under DATA_DIR will be uploaded to the data store\n",
    "DATA = unzip_url(Urls.fridge_objects_path, exist_ok=True)\n",
    "REPS = 3\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "ds.upload(\n",
    "    src_dir=os.path.dirname(DATA),\n",
    "    target_path='data',\n",
    "    overwrite=True,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_folder = os.path.join(os.getcwd(), \"hyperparameter\")\n",
    "print(script_folder)\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))))\n",
    "from azureml.core import Run\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.vision.data import *\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "data_store_path = str(os.environ['AZUREML_DATAREFERENCE_workspaceblobstore'])\n",
    "path = data_store_path + '/data/fridgeObjects'\n",
    "\n",
    "IM_SIZES = [299, 499]\n",
    "ARCHITECTURE  = models.resnet50\n",
    "LEARNING_RATES = [1e-3, 1e-4, 1e-5]\n",
    "EPOCHS = [10]\n",
    "\n",
    "data = (ImageList.from_folder(path)\n",
    "        .split_by_rand_pct(valid_pct=0.2, seed=10)\n",
    "        .label_from_folder() \n",
    "        .transform(size=299) \n",
    "        .databunch(bs=16) \n",
    "        .normalize(imagenet_stats))\n",
    "\n",
    "\n",
    "learn = cnn_learner(\n",
    "    data,\n",
    "    ARCHITECTURE,\n",
    "    metrics=[accuracy]\n",
    "    #callback_fns=[partial(TrainMetricsRecorder, show_graph=True)]\n",
    ")\n",
    "\n",
    "learn.unfreeze()\n",
    "\n",
    "learn.fit(EPOCHS[0], LEARNING_RATES[0])\n",
    "\n",
    "training_losses = [x.numpy().ravel()[0] for x in learn.recorder.losses]\n",
    "accuracy = [x[0].numpy().ravel()[0] for x in learn.recorder.metrics][-1]\n",
    "\n",
    "#run.log_list('training_loss', training_losses)\n",
    "#run.log_list('validation_loss', learn.recorder.val_losses)\n",
    "#run.log_list('error_rate', error_rate)\n",
    "#run.log_list('learning_rate', learn.recorder.lrs)\n",
    "run.log('accuracy', float(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_SIZES = [299, 499]\n",
    "LEARNING_RATES = [1e-3, 1e-4, 1e-5]\n",
    "\n",
    "# Hyperparameter search space\n",
    "param_sampling = RandomParameterSampling( {\n",
    "        'learning_rate': choice(LEARNING_RATES),\n",
    "        'im_sizes': choice(IM_SIZES)\n",
    "    }\n",
    ")\n",
    "\n",
    "primary_metric_name = 'accuracy'\n",
    "primary_metric_goal = PrimaryMetricGoal.MAXIMIZE\n",
    "max_total_runs=50\n",
    "max_concurrent_runs=4\n",
    "\n",
    "early_termination_policy = BanditPolicy(slack_factor=0.15, evaluation_interval=1, delay_evaluation=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    '--data-folder': ds.as_mount(),\n",
    "}\n",
    "\n",
    "est = Estimator(source_directory=script_folder,\n",
    "                script_params=script_params,\n",
    "                compute_target=compute_target,\n",
    "                entry_script='train.py',\n",
    "                pip_packages=['fastai'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'hyperparameter-tuning'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run_config = HyperDriveConfig(estimator=est,\n",
    "                                         hyperparameter_sampling=param_sampling,\n",
    "                                         policy=early_termination_policy,\n",
    "                                         primary_metric_name=primary_metric_name,\n",
    "                                         primary_metric_goal=primary_metric_goal,\n",
    "                                         max_total_runs=max_total_runs,\n",
    "                                         max_concurrent_runs= max_concurrent_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run = exp.submit(config=hyperdrive_run_config)\n",
    "widgets.RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best run and printout metrics\n",
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "parameter_values = best_run.get_details()['runDefinition']['arguments']\n",
    "best_parameters = dict(zip(parameter_values[::2], parameter_values[1::2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"* Best Run Id:\", best_run.id)\n",
    "print(best_run)\n",
    "print(\"\\n* Best hyperparameters:\")\n",
    "print(best_parameters)\n",
    "print(\"Accuracy =\", best_run_metrics['accuracy'])\n",
    "#print(\"Learning Rate =\", best_run_metrics['learning_rate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = best_parameters.copy()\n",
    "model_parameters['--data-folder'] = ds.as_mount()\n",
    "\n",
    "est = Estimator(source_directory=script_folder,\n",
    "                script_params=model_parameters,\n",
    "                compute_target=compute_target,\n",
    "                entry_script='train.py',\n",
    "                pip_packages=['fastai'])\n",
    "\n",
    "model_run = exp.submit(est)\n",
    "model_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
