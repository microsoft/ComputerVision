{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image annotation UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open-source annotation tools for object detection and for image segmentation exist, however for image classification we were not able to find a good program. Hence this notebook provides a simple UI to label images. Each image can be annotated with one or multiple classes, or marked as \"Exclude\" to indicate that the image should not be used for model trainining or evaluation. \n",
    "\n",
    "Note that, for single class annotation tasks, one does not need any UI but can instead simply drag&drop images into separate folder for the respective classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensure edits to libraries are loaded and plotting is shown in the notebook.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils_ic.anno_utils import AnnotationWidget\n",
    "from utils_ic.datasets import unzip_url, Urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters: location of the images to annotate, and path where to save the annotations. Here `unzip_url` is used to download example data if not already present, and set the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using images in directory: C:\\Users\\pabuehle\\Desktop\\ComputerVisionBestPractices\\image_classification\\data\\fridgeObjects\\can.\n"
     ]
    }
   ],
   "source": [
    "IM_DIR = os.path.join((unzip_url(Urls.fridge_objects_path, exist_ok=True)), 'can')\n",
    "ANNO_PATH = \"cvbp_ic_annotation.pkl\"\n",
    "print(f\"Using images in directory: {IM_DIR}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the UI. Check the \"Allow multi-class labeling\" box to allow that images can be annotated as multiple classes. When in doubt what the annotation for an image should be, or for any other reason (e.g. blur or over-exposure), mark an image as \"EXCLUDE\". All annotations are saved to (and loaded from) a pandas dataframe with path specified in `anno_path`. \n",
    "\n",
    "<center>\n",
    "<img src=\"https://cvbp.blob.core.windows.net/public/images/document_images/anno_ui2.jpg\" style=\"width: 600px;\"/>\n",
    "<i>Annotation UI example</i>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b16002f1454378a00c28e8b06d12a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(VBox(children=(HBox(children=(Button(description='Previous', layout=Layout(width='80px'), style=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w_anno_ui = AnnotationWidget(\n",
    "    labels       = [\"can\", \"carton\", \"milk_bottle\", \"water_bottle\"],\n",
    "    im_dir       = IM_DIR,\n",
    "    anno_path    = ANNO_PATH,\n",
    "    im_filenames = None #Set to None to annotate all images in IM_DIR\n",
    ")\n",
    "\n",
    "display(w_anno_ui.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example how to create a fast.ai ImageList object using the ground truth annotations generated by the AnnotationWidget. Note that fast.ai does not support the exclude flag, hence we remove these images before calling fast.ai's `from_df()` and `label_from_df()` functions.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from fastai.vision import ImageList,ImageDataBunch\n",
    "\n",
    "# Load annotation, discard excluded images, and convert to format fastai expects\n",
    "annos = pd.read_pickle(ANNO_PATH) \n",
    "keys = [key for key in annos if (not annos[key].exclude and len(annos[key].labels)>0)]\n",
    "df = pd.DataFrame([(anno[0], \",\".join(anno[1].labels)) for anno in annos[keys].items()], \n",
    "                  columns = [\"name\", \"label\"])\n",
    "display(df)\n",
    "\n",
    "# Create an ImageList and assign labels \n",
    "data = (ImageList.from_df(path=IM_DIR, df = df)\n",
    "       .split_by_rand_pct(valid_pct=0.5)\n",
    "       .label_from_df(label_delim=','))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Image Scraping and Annotation\n",
    "\n",
    "Collecting a sufficiently large number of annotated images for training and testing can be difficult. One way to over-come this problem is to scrape images from the Internet. For example, see below the Bing Image Search results for the query \"tshirt striped\". As expected, most images indeed are striped t-shirts, and the few incorrect or ambiguous images (such as column 1, row 1; or column 3, row 2) can be identified and removed easily:\n",
    "<p align=\"center\">\n",
    "<img src=\"media/bing_search_striped.jpg\" alt=\"alt text\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "To generate a large and diverse dataset, multiple queries should be used. For example 7\\*3 = 21 queries can by synthesized automatically using all combinations of 7 clothing items {blouse, hoodie, pullover, sweater, shirt, tshirt, vest} and 3 attributes {striped, dotted, leopard}. Downloading the top 50 images per query would then lead to a maximum of 21*50=1050 images.\n",
    "\n",
    "Rather than manually downloading images from Bing Image Search, it is much easier to instead use the [Cognitive Services Bing Image Search API](https://www.microsoft.com/cognitive-services/en-us/bing-image-search-api) which returns a set of image URLs given a query string:\n",
    "<p align=\"center\">\n",
    "<img src=\"media/bing_image_search_api.jpg\" alt=\"alt text\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "Some of the downloaded images will be exact or near duplicates (e.g. differ just by image resolution or jpg artifacts) and should be removed so that the training and test split do not contain the same images. This can be achieved using a hashing-based approach which works in two steps: (i) first, the hash string is computed for all images; (ii) in a second pass over the images, only those are kept with a hash string which has not yet been seen. All other images are discarded. We found the *dhash* approach in the Python library *imagehash* and described in this [blog](http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html) to perform well, with the parameter `hash_size` set to 16. It is OK to incorrectly remove some non-duplicates images, as long as the majority of the real duplicates get removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cvbp)",
   "language": "python",
   "name": "cvbp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
