{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO (make changes also in image classification notebook to stay in sync):\n",
    "- Try setting env via the \"conda_dependencies_file\" parameter [link](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.estimator.estimator?view=azure-ml-py)\n",
    "- Why neg ap's and not identical with print-out\n",
    "- Could upgrade to cuda10\n",
    "\n",
    "TODO image classification only:\n",
    "- Change to only upload the relevant data, and replace hard-coded foldername in training code.\n",
    "- Fix display of _STANDARD_DS6_ etc machines\n",
    "- Change output dir from \"outputs\" to: output_folder = os.path.join(current_directory, 'hyperdrive_outputs')\n",
    "- Rename DATA to DATA_PATH\n",
    "- Rename script_folder to hyperdrive from hyperparameters\n",
    "- Add \"use_gpu=True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different Hyperparameters and Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll cover how to test different hyperparameters for a particular dataset and how to benchmark different parameters across a group of datasets using AzureML. We assume familiarity with the basic concepts and parameters, which are discussed in the [01_training_introduction.ipynb](01_training_introduction.ipynb), [02_mask_rcnn.ipynb](02_mask_rcnn.ipynb) and [03_training_accuracy_vs_speed.ipynb](03_training_accuracy_vs_speed.ipynb) notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the image classification notebook [11_exploring_hyperparameters.ipynb](../../classification/notebooks/11_exploring_hyperparameters.ipynb), we will learn more about how different learning rates and different image sizes affect our model's accuracy when restricted to 16 epochs, and we want to build an AzureML experiment to test out these hyperparameters. \n",
    "\n",
    "We will be using a Faster R-CNN model with ResNet-50 backbone to find all objects in an image belonging to 4 categories: 'can', 'carton', 'milk_bottle', 'water_bottle'. We will then conduct hyper-parameter tuning to find the best set of parameters for this model. For this, we present an overall process of utilizing AzureML, specifically [Hyperdrive](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive?view=azure-ml-py) component to run this tuning in parallel (and not successively).We demonstrate the following key steps:  \n",
    "* Configure AzureML Workspace\n",
    "* Create Remote Compute Target (GPU cluster)\n",
    "* Prepare Data\n",
    "* Prepare Training Script\n",
    "* Setup and Run Hyperdrive Experiment\n",
    "* Model Import, Re-train and Test\n",
    "\n",
    "This notebook is very similar to the [24_exploring_hyperparameters_on_azureml.ipynb](../../classification/notebooks/24_exploring_hyperparameters_on_azureml.ipynb) hyperdrive notebook used for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from distutils.dir_util import copy_tree\n",
    "import numpy as np\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "import azureml.data\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.train.hyperdrive import (\n",
    "    RandomParameterSampling, GridParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal, choice, uniform\n",
    ")\n",
    "import azureml.widgets as widgets\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from utils_cv.common.data import unzip_url\n",
    "from utils_cv.detection.data import Urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure edits to libraries are loaded and plotting is shown in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define some parameters which will be used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Azure resources\n",
    "# subscription_id = \"YOUR_SUBSCRIPTION_ID\"\n",
    "# resource_group = \"YOUR_RESOURCE_GROUP_NAME\"  \n",
    "# workspace_name = \"YOUR_WORKSPACE_NAME\"  \n",
    "# workspace_region = \"YOUR_WORKSPACE_REGION\" #Possible values eastus, eastus2, etc.\n",
    "subscription_id = \"2ad17db4-e26d-4c9e-999e-adae9182530c\"  #Sharat \n",
    "resource_group = \"pabuehle_delme2_hyperdrive\"  \n",
    "workspace_name = \"pabuehle_ws\"  \n",
    "workspace_region = \"eastus\" #Possible values eastus, eastus2, etc.\n",
    "\n",
    "# Choose a size for our cluster and the maximum number of nodes\n",
    "VM_SIZE = \"STANDARD_NC6\" #\"STANDARD_NC6\", STANDARD_NC6S_V3\"\n",
    "MAX_NODES = 5 #12\n",
    "\n",
    "# Hyperparameter search space\n",
    "IM_MAX_SIZES = [50] #[150, 300]\n",
    "LEARNING_RATE_MAX = 1e-2\n",
    "LEARNING_RATE_MIN = 1e-6\n",
    "MAX_TOTAL_RUNS = 10 #Set to higher value to test more parameter combinations.\n",
    "\n",
    "# Image data\n",
    "#DATA = unzip_url(Urls.fridge_objects_path, exist_ok=True)\n",
    "DATA_PATH = \"C:/Users/pabuehle/Desktop/ComputerVision/data/odFridgeObjectsTiny\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Config AzureML workspace\n",
    "Below we setup (or load an existing) AzureML workspace, and get all its details as follows. Note that the resource group and workspace will get created if they do not yet exist. For more information regaring the AzureML workspace see also the [20_azure_workspace_setup.ipynb](../../classification/notebooks/20_azure_workspace_setup.ipynb) notebook in the image classification folder.\n",
    "\n",
    "To simplify clean-up (see end of this notebook), we recommend creating a new resource group to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR - get_workspace error using subscription_id=2ad17db4-e26d-4c9e-999e-adae9182530c, resource_group_name=pabuehle_delme2_hyperdrive, workspace_name=pabuehle_ws\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new workspace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The resource group doesn't exist or was not provided. AzureML SDK is creating a resource group=pabuehle_delme2_hyperdrive in location=eastus using subscription=2ad17db4-e26d-4c9e-999e-adae9182530c.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying AppInsights with name pabuehleinsights13d0d903.\n",
      "Deployed AppInsights with name pabuehleinsights13d0d903. Took 20.23 seconds.\n",
      "Deploying KeyVault with name pabuehlekeyvaulta50e31a0.\n",
      "Deploying StorageAccount with name pabuehlestorage33ceb94ce.\n",
      "Deployed KeyVault with name pabuehlekeyvaulta50e31a0. Took 37.19 seconds.\n",
      "Deployed StorageAccount with name pabuehlestorage33ceb94ce. Took 38.13 seconds.\n",
      "Deploying Workspace with name pabuehle_ws.\n",
      "Deployed Workspace with name pabuehle_ws. Took 60.59 seconds.\n",
      "Workspace name: pabuehle_ws\n",
      "Workspace region: eastus\n",
      "Subscription id: 2ad17db4-e26d-4c9e-999e-adae9182530c\n",
      "Resource group: pabuehle_delme2_hyperdrive\n"
     ]
    }
   ],
   "source": [
    "from utils_cv.common.azureml import get_or_create_workspace\n",
    "\n",
    "ws = get_or_create_workspace(\n",
    "        subscription_id,\n",
    "        resource_group,\n",
    "        workspace_name,\n",
    "        workspace_region)\n",
    "\n",
    "# Print the workspace attributes\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Workspace region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Remote Target\n",
    "We create a GPU cluster as our remote compute target. If a cluster with the same name already exists in our workspace, the script will load it instead. This [link](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#compute-targets-for-training) provides more information about how to set up a compute target on different locations.\n",
    "\n",
    "By default, the VM size is set to use STANDARD\\_NC6 machines. However, if quota is available, our recommendation is to use STANDARD\\_NC6S\\_V3 machines which come with the much faster V100 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new compute target...\n",
      "Creating\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n",
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-08-30T00:13:31.543000+00:00', 'errors': None, 'creationTime': '2019-08-30T00:13:28.454854+00:00', 'modifiedTime': '2019-08-30T00:13:44.726668+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 5, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "CLUSTER_NAME = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    # Retrieve if a compute target with the same cluster name already exists\n",
    "    compute_target = ComputeTarget(workspace=ws, name=CLUSTER_NAME)\n",
    "    print('Found existing compute target.')\n",
    "    \n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, we create a new one with the name provided\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=VM_SIZE,\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=MAX_NODES)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, CLUSTER_NAME, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# we can use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare data\n",
    "In this notebook, we'll use the Fridge Objects dataset, which is already stored in the correct format. We then upload our data to the AzureML workspace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 8 files\n",
      "Uploading C:/Users/pabuehle/Desktop/ComputerVision/data/odFridgeObjectsTiny\\annotations\\35.xml\n",
      "Uploading C:/Users/pabuehle/Desktop/ComputerVision/data/odFridgeObjectsTiny\\annotations\\40.xml\n",
      "Uploading C:/Users/pabuehle/Desktop/ComputerVision/data/odFridgeObjectsTiny\\annotations\\45.xml\n",
      "Uploading C:/Users/pabuehle/Desktop/ComputerVision/data/odFridgeObjectsTiny\\annotations\\50.xml\n",
      "Uploading C:/Users/pabuehle/Desktop/ComputerVision/data/odFridgeObjectsTiny\\images\\35.jpg\n",
      "Uploading C:/Users/pabuehle/Desktop/ComputerVision/data/odFridgeObjectsTiny\\images\\40.jpg\n",
      "Uploading C:/Users/pabuehle/Desktop/ComputerVision/data/odFridgeObjectsTiny\\images\\45.jpg\n",
      "Uploading C:/Users/pabuehle/Desktop/ComputerVision/data/odFridgeObjectsTiny\\images\\50.jpg\n",
      "Uploaded C:/Users/pabuehle/Desktop/ComputerVision/data/odFridgeObjectsTiny\\annotations\\45.xml, 1 files out of an estimated total of 8\n",
      "Uploaded C:/Users/pabuehle/Desktop/ComputerVision/data/odFridgeObjectsTiny\\annotations\\40.xml, 2 files out of an estimated total of 8\n",
      "Uploaded C:/Users/pabuehle/Desktop/ComputerVision/data/odFridgeObjectsTiny\\annotations\\35.xml, 3 files out of an estimated total of 8\n",
      "Uploaded C:/Users/pabuehle/Desktop/ComputerVision/data/odFridgeObjectsTiny\\annotations\\50.xml, 4 files out of an estimated total of 8\n",
      "Uploaded C:/Users/pabuehle/Desktop/ComputerVision/data/odFridgeObjectsTiny\\images\\45.jpg, 5 files out of an estimated total of 8\n",
      "Uploaded C:/Users/pabuehle/Desktop/ComputerVision/data/odFridgeObjectsTiny\\images\\50.jpg, 6 files out of an estimated total of 8\n",
      "Uploaded C:/Users/pabuehle/Desktop/ComputerVision/data/odFridgeObjectsTiny\\images\\35.jpg, 7 files out of an estimated total of 8\n",
      "Uploaded C:/Users/pabuehle/Desktop/ComputerVision/data/odFridgeObjectsTiny\\images\\40.jpg, 8 files out of an estimated total of 8\n",
      "Uploaded 8 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_97dd0cd1aec6415f8c6f7f2587601881"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieving default datastore that got automatically created when we setup a workspace\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "# We now upload the data to the 'data' folder on the Azure portal\n",
    "ds.upload(\n",
    "    src_dir=DATA_PATH,\n",
    "    target_path='data',\n",
    "    overwrite=True, # with \"overwrite=True\", if this data already exists on the Azure blob storage, it will be overwritten\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here's where you can see the data in your portal: \n",
    "<img src=\"media/datastore.jpg\" width=\"800\" alt=\"Datastore screenshot for Hyperdrive notebook run\">\n",
    "\n",
    "### 4. Prepare training script\n",
    "\n",
    "Next step is to prepare scripts that AzureML Hyperdrive will use to train and evaluate models with selected hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for the training script and the utils_cv library\n",
    "script_folder = os.path.join(os.getcwd(), \"hyperdrive\")\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy utils_cv library to script folder\n",
    "_ = copy_tree(os.path.join('..', '..', 'utils_cv'), os.path.join(script_folder, 'utils_cv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:\\Users\\pabuehle\\Desktop\\ComputerVision\\detection\\notebooks\\hyperdrive/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "# Use different matplotlib backend to avoid error during remote execution\n",
    "import matplotlib \n",
    "matplotlib.use(\"Agg\") \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from azureml.core import Run\n",
    "from utils_cv.detection.dataset import DetectionDataset\n",
    "from utils_cv.detection.model import DetectionLearner \n",
    "from utils_cv.common.gpu import which_processor\n",
    "which_processor()\n",
    "\n",
    "\n",
    "# Parse arguments passed by Hyperdrive\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=Path, dest='data_dir')\n",
    "parser.add_argument('--epochs', type=int, dest='epochs', default=10)\n",
    "parser.add_argument('--batch_size', type=int, dest='batch_size', default=1)\n",
    "parser.add_argument('--learning_rate', type=float, dest='learning_rate', default=1e-4)\n",
    "parser.add_argument('--min_size', type=int, dest='min_size', default=800)\n",
    "parser.add_argument('--max_size', type=int, dest='max_size', default=1333)\n",
    "parser.add_argument('--rpn_pre_nms_top_n_train', type=int, dest='rpn_pre_nms_top_n_train', default=2000)\n",
    "parser.add_argument('--rpn_pre_nms_top_n_test', type=int, dest='rpn_pre_nms_top_n_test', default=1000)\n",
    "parser.add_argument('--rpn_post_nms_top_n_train', type=int, dest='rpn_post_nms_top_n_train', default=2000)\n",
    "parser.add_argument('--rpn_post_nms_top_n_test', type=int, dest='rpn_post_nms_top_n_test', default=1000)\n",
    "parser.add_argument('--rpn_nms_thresh', type=float, dest='rpn_nms_thresh', default=0.7)\n",
    "parser.add_argument('--box_score_thresh', type=float, dest='box_score_thresh', default=0.05)\n",
    "parser.add_argument('--box_nms_thresh', type=float, dest='box_nms_thresh', default=0.5)\n",
    "parser.add_argument('--box_detections_per_img', type=int, dest='box_detections_per_img', default=100)\n",
    "parser.add_argument('-f', dest='dummy', default=\"dummy\")\n",
    "args = parser.parse_args()\n",
    "params = vars(args)\n",
    "print(f\"params = {params}\")\n",
    "\n",
    "\n",
    "#path = \"C:/Users/pabuehle/Desktop/ComputerVision/data/odFridgeObjectsTiny/\"\n",
    "#params['epochs'] = 1\n",
    "\n",
    "\n",
    "# Getting training and validation data\n",
    "path = params['data_dir'] + \"/data/\"\n",
    "data = DetectionDataset(path, train_pct=0.5, batch_size = params[\"batch_size\"])\n",
    "print(\n",
    "    f\"Training dataset: {len(data.train_ds)} | Training DataLoader: {data.train_dl} \\n \\\n",
    "    Testing dataset: {len(data.test_ds)} | Testing DataLoader: {data.test_dl}\"\n",
    ")\n",
    "\n",
    "# Get model\n",
    "detector = DetectionLearner(data,\n",
    "    min_size = params[\"min_size\"],\n",
    "    max_size = params[\"max_size\"],\n",
    "    rpn_pre_nms_top_n_train = params[\"rpn_pre_nms_top_n_train\"],\n",
    "    rpn_pre_nms_top_n_test = params[\"rpn_pre_nms_top_n_test\"],\n",
    "    rpn_post_nms_top_n_train = params[\"rpn_post_nms_top_n_train\"], \n",
    "    rpn_post_nms_top_n_test = params[\"rpn_post_nms_top_n_test\"],\n",
    "    rpn_nms_thresh = params[\"rpn_nms_thresh\"],\n",
    "    box_score_thresh = params[\"box_score_thresh\"], \n",
    "    box_nms_thresh = params[\"box_nms_thresh\"],\n",
    "    box_detections_per_img = params[\"box_detections_per_img\"]\n",
    ")\n",
    "\n",
    "# Run Training\n",
    "detector.fit(params[\"epochs\"], lr=params[\"learning_rate\"], print_freq=30)\n",
    "print(f\"Average precision after each epoch: {detector.ap}\")\n",
    "\n",
    "# Add log entries\n",
    "run = Run.get_context()\n",
    "run.log(\"data_dir\", params[\"data_dir\"])\n",
    "run.log(\"min_size\", params[\"min_size\"])\n",
    "run.log(\"learning_rate\", params[\"learning_rate\"])\n",
    "run.log(\"accuracy\", float(detector.ap[-1]))  # Logging our primary metric 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Setup and run Hyperdrive experiment\n",
    "\n",
    "#### 5.1 Create Experiment  \n",
    "Experiment is the main entry point into experimenting with AzureML. To create new Experiment or get the existing one, we pass our experimentation name 'hyperparameter-tuning'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'hyperparameter-tuning'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Define search space\n",
    "\n",
    "Now we define the search space of hyperparameters. As shown below, to test discrete parameter values use 'choice()', and for uniform sampling use 'uniform()'. For more options, see [Hyperdrive parameter expressions](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.parameter_expressions?view=azure-ml-py).\n",
    "\n",
    "Hyperdrive provides three different parameter sampling methods: 'RandomParameterSampling', 'GridParameterSampling', and 'BayesianParameterSampling'. Details about each method can be found [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters). Here, we use the 'RandomParameterSampling'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1e-06, 0.005000500000000001, 0.01]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrs = [float(f) for f in np.linspace(LEARNING_RATE_MIN, LEARNING_RATE_MAX, 3)]\n",
    "lrs\n",
    "#lrs = list(np.linspace(LEARNING_RATE_MIN, LEARNING_RATE_MAX, 3).astype(float))\n",
    "#type(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search space\n",
    "# param_sampling = RandomParameterSampling( {\n",
    "#         '--learning_rate': uniform(LEARNING_RATE_MIN, LEARNING_RATE_MAX),\n",
    "#         '--im_size': choice(IM_SIZES)\n",
    "#     }\n",
    "# )\n",
    "#early_termination_policy = BanditPolicy(slack_factor=0.15, evaluation_interval=1, delay_evaluation=20)\n",
    "\n",
    "\n",
    "\n",
    "# Grid-search\n",
    "param_sampling = GridParameterSampling( {\n",
    "        '--learning_rate': choice(lrs),\n",
    "        '--im_size': choice(IM_MAX_SIZES)\n",
    "    }\n",
    ")\n",
    "early_termination_policy = None\n",
    "MAX_TOTAL_RUNS = None # Set to zero to run all possible grid parameter combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>AzureML Estimator</b> is the building block for training. An Estimator encapsulates the training code and parameters, the compute resources and runtime environment for a particular training scenario.\n",
    "We create one for our experimentation with the dependencies our model requires as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    '--data-folder': ds.as_mount()\n",
    "}\n",
    "\n",
    "est = Estimator(source_directory=script_folder,\n",
    "                script_params=script_params,\n",
    "                compute_target=compute_target,\n",
    "                entry_script='train.py',\n",
    "                use_gpu=True,\n",
    "                pip_packages=['nvidia-ml-py3','fastai'],\n",
    "                conda_packages=['scikit-learn', 'pycocotools>=2.0','torchvision==0.3','cudatoolkit==9.0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a HyperDriveConfig object which  includes information about parameter space sampling, termination policy, primary metric, estimator and the compute target to execute the experiment runs on. We feed the following parameters to it:\n",
    "\n",
    "- our estimator object that we created in the above cell\n",
    "- hyperparameter sampling method, in this case it is Random Parameter Sampling\n",
    "- early termination policy, in this case we use [Bandit Policy](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters#bandit-policy)\n",
    "- primary metric name reported by our runs, in this case it is accuracy \n",
    "- the goal, which determines whether the primary metric has to be maximized/minimized, in this case it is to maximize our accuracy \n",
    "- number of total child-runs\n",
    "\n",
    "The bigger the search space, the more child-runs get triggered for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run_config = HyperDriveConfig(estimator=est,\n",
    "                                         hyperparameter_sampling=param_sampling,\n",
    "                                         policy=early_termination_policy,\n",
    "                                         primary_metric_name='accuracy',\n",
    "                                         primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                         max_total_runs=MAX_TOTAL_RUNS,\n",
    "                                         max_concurrent_runs=MAX_NODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81c97a6ecc64c238e8b6276e0d61fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we submit the Run to our experiment. \n",
    "hyperdrive_run = exp.submit(config=hyperdrive_run_config)\n",
    "\n",
    "# We can see the experiment progress from this notebook by using \n",
    "widgets.RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can check from the Azure portal with the url link we get by running \n",
    "```python \n",
    "hyperdrive_run.get_portal_url().```\n",
    "\n",
    "To load an existing Hyperdrive Run instead of start new one, we can use \n",
    "```python\n",
    "hyperdrive_run = azureml.train.hyperdrive.HyperDriveRun(exp, <your-run-id>, hyperdrive_run_config=hyperdrive_run_config)\n",
    "```\n",
    "We also can cancel the Run with \n",
    "```python \n",
    "hyperdrive_run_config.cancel().\n",
    "```\n",
    "\n",
    "Once all the child-runs are finished, we can get the best run and the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get best run and print out metrics\n",
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "parameter_values = best_run.get_details()['runDefinition']['arguments']\n",
    "best_parameters = dict(zip(parameter_values[::2], parameter_values[1::2]))\n",
    "\n",
    "print(f\"* Best Run Id:{best_run.id}\")\n",
    "print(best_run)\n",
    "print(\"\\n* Best hyperparameters:\")\n",
    "print(best_parameters)\n",
    "print(f\"Accuracy = {best_run_metrics['accuracy']}\")\n",
    "#print(\"Learning Rate =\", best_run_metrics['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Clean up\n",
    "\n",
    "To avoid unnecessary expenses, all resources which were created in this notebook need to get deleted once parameter search is concluded. To simplify this clean-up step, we recommend creating a new resource group to run this notebook. This resource group can then be deleted, e.g. using the Azure Portal, which will remove all created resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tv2)",
   "language": "python",
   "name": "tv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
