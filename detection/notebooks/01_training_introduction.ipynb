{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Object Detection Model\n",
    "\n",
    "In this notebook, we give an introduction to training an object detection model using [torchvision](https://pytorch.org/docs/stable/torchvision/index.html). Using a small dataset, we demonstrate how to train and evaluate a FasterRCNN object detection model. We also cover one of the most common ways to store data on a file system for this type of problem.\n",
    "\n",
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure edits to libraries are loaded and plotting is shown in the notebook.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "# %load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the functions we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from random import randrange\n",
    "from typing import Tuple, List\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils_cv.common.data import unzip_url, data_path\n",
    "from utils_cv.detection.data import Urls\n",
    "from utils_cv.detection.dataset import DetectionDataset\n",
    "from utils_cv.detection.plot import display_bounding_boxes\n",
    "from utils_cv.detection.model import get_bounding_boxes, get_transform, get_pretrained_fasterrcnn\n",
    "from utils_cv.detection.references.engine import train_one_epoch, evaluate\n",
    "from utils_cv.detection.references.utils import collate_fn\n",
    "from utils_cv.common.gpu import which_processor\n",
    "\n",
    "which_processor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows your machine's GPUs (if has any) and the computing device `torch/torchvision` is using. We suggest using an  [Azure DSVM](https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/) Standard NC6 as a GPU compute resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set some model runtime parameters. We use the `unzip_url` helper function to download and unzip the data used in this example notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH     = unzip_url(Urls.fridge_objects_path, exist_ok=True)\n",
    "EPOCHS        = 3\n",
    "LEARNING_RATE = 0.005\n",
    "# IM_SIZE       = 300\n",
    "# BATCH_SIZE    = 16\n",
    "# ARCHITECTURE  = models.resnet18\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Prepare Object Detection Dataset\n",
    "\n",
    "In this notebook, we use a toy dataset called *Fridge Objects*, which consists of 134 images of 4 classes of beverage container `{can, carton, milk bottle, water bottle}` photos taken on different backgrounds. The helper function downloads and unzips data set to the `ComputerVision/data` directory.\n",
    "\n",
    "Set that directory in the `path` variable for ease of use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(DATA_PATH)\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that we have two different folders inside:\n",
    "- `/images`\n",
    "- `/annotations`\n",
    "\n",
    "This format for object detection is fairly common.\n",
    "\n",
    "```\n",
    "/data\n",
    "+-- images\n",
    "|   +-- image1.jpg\n",
    "|   +-- image2.jpg\n",
    "|   +-- ...\n",
    "+-- annotations\n",
    "|   +-- image1.xml\n",
    "|   +-- image2.xml\n",
    "|   +-- ...\n",
    "+-- ...\n",
    "```\n",
    "\n",
    "The xml files inside the annotation folder contain information on where the objects are the xml's corresponding image file. In this example, our fridge object dataset is annotated in Pascal VOC format. This is one of the most common formats for labelling object detection datasets.\n",
    "\n",
    "```xml\n",
    "<!-- Example Pascal VOC annotation -->\n",
    "<annotation>\n",
    "    <folder>images</folder>\n",
    "    <filename>1.jpg</filename>\n",
    "    <path>../images/1.jpg</path>\n",
    "    <source>\n",
    "        <database>Unknown</database>\n",
    "    </source>\n",
    "    <size>\n",
    "        <width>499</width>\n",
    "        <height>666</height>\n",
    "        <depth>3</depth>\n",
    "    </size>\n",
    "    <segmented>0</segmented>\n",
    "\n",
    "    <object>\n",
    "        <name>carton</name>\n",
    "        <pose>Unspecified</pose>\n",
    "        <truncated>0</truncated>\n",
    "        <difficult>0</difficult>\n",
    "        <bndbox>\n",
    "            <xmin>100</xmin>\n",
    "            <ymin>173</ymin>\n",
    "            <xmax>233</xmax>\n",
    "            <ymax>521</ymax>\n",
    "        </bndbox>\n",
    "    </object>\n",
    "</annotation>\n",
    "```\n",
    "\n",
    "You'll notice that inside the annotation xml file, we can see which image the file references `<path>`, the number of `<objects>` in the image, that the image is of (`<name>`) and the bounding box of that object (`<bndbox>`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Images\n",
    "\n",
    "To load the data, we need to create a Dataset object class that Torchvision knows how to use. In short, we'll need to create a class and implement the `__getitem__` method. More information here: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html#defining-the-dataset\n",
    "\n",
    "To make it more convinient, we've created a `DetectionDataset` class that knows how to extract annotation information from the Pascal VOC format and meet the requirements of the Torchvision dataset object class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fridge_objects_dataset = DetectionDataset(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize some of the images to make sure the annotation looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fridge_objects_dataset.show_batch(rows=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset setup, we want to split it into train/test sets, and then use them to create our data loaders which will be used when fine-tuning the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset in train and test set\n",
    "train_ds, test_ds = fridge_objects_dataset.split_train_test()\n",
    "\n",
    "# define training and validation data loaders\n",
    "train_dl = DataLoader(\n",
    "    train_ds, batch_size=2, shuffle=True, num_workers=4, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    test_ds, batch_size=1, shuffle=False, num_workers=4, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make sure that the number of images in the training and testing set is what is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\\\n",
    "Training images: {len(train_ds)}\n",
    "Testing images: {len(test_ds)}\\\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune a Pretrained Model\n",
    "\n",
    "For this object detector, we use FasterRCNN, and Stochastic Gradient Descent as our optimizer. Our FasterRCNN model is pretrained on COCO, a large-scale object detection, segmentation, and captioning dataset that contains over 200K labeled images with over 80 label cateogories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model using our helper function\n",
    "model = get_pretrained_fasterrcnn(len(fridge_objects_dataset.categories)).to(device)\n",
    "\n",
    "# construct our optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fine-tune the model using our training data loader (`train_dl`) and evaluate our results using our testing data loader (`test_dl`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n",
    "    def f(x):\n",
    "        if x >= warmup_iters:\n",
    "            return 1\n",
    "        alpha = float(x) / warmup_iters\n",
    "        return warmup_factor * (1 - alpha) + alpha\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n",
    "\n",
    "\n",
    "def reduce_dict(input_dict, average=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_dict (dict): all the values will be reduced\n",
    "        average (bool): whether to do average or sum\n",
    "    Reduce the values in the dictionary from all processes so that all processes\n",
    "    have the averaged results. Returns a dict with the same fields as\n",
    "    input_dict, after reduction.\n",
    "    \"\"\"\n",
    "    world_size = 1 #get_world_size()\n",
    "    if world_size < 2:\n",
    "        return input_dict\n",
    "    with torch.no_grad():\n",
    "        names = []\n",
    "        values = []\n",
    "        # sort the keys so that they are consistent across processes\n",
    "        for k in sorted(input_dict.keys()):\n",
    "            names.append(k)\n",
    "            values.append(input_dict[k])\n",
    "        values = torch.stack(values, dim=0)\n",
    "        dist.all_reduce(values)\n",
    "        if average:\n",
    "            values /= world_size\n",
    "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
    "    return reduced_dict\n",
    "\n",
    "def train_one_epoch2(model, optimizer, lr_scheduler, data_loader, device, epoch, print_freq):\n",
    "    model.train()\n",
    "    header = \"Epoch: [{}]\".format(epoch)\n",
    "    all_losses = []\n",
    "\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = warmup_lr_scheduler(\n",
    "            optimizer, warmup_iters, warmup_factor\n",
    "        )\n",
    "\n",
    "    for idx, (images, targets) in enumerate(data_loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "        all_losses.append(losses_reduced.item())\n",
    "\n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, lr_scheduler, train_dl, device, epoch, print_freq=10)\n",
    "    \n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = evaluate(model, test_dl, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select image\n",
    "im_path = test_ds.dataset.root/'images'/test_ds.dataset.ims[randrange(len(test_ds))]\n",
    "im = Image.open(im_path)\n",
    "\n",
    "# Define PyTorch Transform\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Apply the transform to the image\n",
    "im = transform(im).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    pred = model([im])\n",
    "    print(f\"Time spend: {time.time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels, pred_boxes = get_bounding_boxes(pred)\n",
    "display_bounding_boxes(pred_boxes, [fridge_objects_dataset.categories[l] for l in pred_labels], im_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fridge_objects_dataset.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cv)",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
