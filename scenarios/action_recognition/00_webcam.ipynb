{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2bd54cdb-aa39-48f9-8bb0-d602135c1b6c"
    }
   },
   "source": [
    "# R(2+1)D Model on Webcam Stream\n",
    "\n",
    "## Prerequisite for Webcam example\n",
    "This notebook assumes you have a webcam connected to your machine. If you want to use a remote-VM to run the model and codes while using a local machine for the webcam stream, you can use an SSH tunnel:\n",
    "\n",
    "1. SSH connect to your VM:\n",
    "`$ ssh -L 8888:localhost:8888 <user-id@url-to-your-vm>`\n",
    "1. Launch a Jupyter session on the VM (with port 8888 which is the default)\n",
    "1. Open localhost:8888 from your browser on the webcam connected local machine to access the Jupyter notebook running on the VM.\n",
    "\n",
    "We use the `ipywebrtc` module to show the webcam widget in the notebook. Currently, the widget works on Chrome and Firefox. For more details about the widget, please visit [ipywebrtc github](https://github.com/maartenbreddels/ipywebrtc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6db47566-8c49-44bb-aa28-c63bf3fb63cd"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "89d4b446-c421-488c-8967-8a38e538a9b2"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from collections import deque\n",
    "import io\n",
    "import os\n",
    "from time import sleep, time\n",
    "from threading import Thread\n",
    "\n",
    "import decord\n",
    "import IPython.display\n",
    "from ipywebrtc import CameraStream, ImageRecorder\n",
    "from ipywidgets import HBox, HTML, Layout, VBox, Widget, Label\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.cuda as cuda\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from utils_cv.action_recognition.data import KINETICS\n",
    "from utils_cv.action_recognition.model import R2Plus1D \n",
    "from utils_cv.action_recognition import system_info, transforms_video as transforms\n",
    "\n",
    "system_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "cf40a98f-414d-4827-8aa8-88ef6b9cc626"
    }
   },
   "source": [
    "## Load Pre-trained Model\n",
    "\n",
    "Load R(2+1)D 34-layer model pre-trained on IG65M and fine-tuned on Kinetics400. There are two versions of the model: 8-frame model and 32-frame model based on the input clip length. The 32-frame model is slower than 8-frame model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c456a2db-b030-46fc-8cd7-55c133feb354"
    }
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 400\n",
    "NUM_FRAMES = 8    # 8 or 32.\n",
    "IM_SCALE = 128    # resize then crop\n",
    "INPUT_SIZE = 112  # input clip size: 3 x NUM_FRAMES x 112 x 112\n",
    "# Normalization\n",
    "MEAN = (0.43216, 0.394666, 0.37645)\n",
    "STD = (0.22803, 0.22145, 0.216989)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c6f4b85d-2f55-4d5a-8edb-5a9b9f8cc8b4"
    }
   },
   "outputs": [],
   "source": [
    "model = R2Plus1D.init_model(\n",
    "    sample_length=NUM_FRAMES,\n",
    "    base_model='kinetics'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8cec9405-d125-4681-9bdd-ec2184257de3"
    }
   },
   "source": [
    "### Prepare class names\n",
    "Since we use Kinetics400 model out of the box, we load its class names. The dataset consists of 400 human actions. For example, the first 20 labels are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "eb1dfefe-dfe4-46e4-8482-9b2bc1d05b89"
    }
   },
   "outputs": [],
   "source": [
    "labels = KINETICS.class_names\n",
    "labels[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5e7de627-d6db-422c-a406-91d15c93985c"
    }
   },
   "source": [
    "Among them, we will use 50 classes that we are interested in (i.e. the actions make sense to demonstrate in front of the webcam) and ignore other classes by filtering out from the model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "cb834095-db67-4b4b-87c3-a133a848a484"
    }
   },
   "outputs": [],
   "source": [
    "REL_LABELS = [\n",
    "    \"assembling computer\",\n",
    "    \"applying cream\",\n",
    "    \"brushing teeth\",\n",
    "    \"clapping\",\n",
    "    \"cleaning floor\",\n",
    "    \"cleaning windows\",\n",
    "    \"drinking\",\n",
    "    # will regard all eatings as simply \"eating\"\n",
    "    \"eating burger\",\n",
    "    \"eating chips\",\n",
    "    \"eating doughnuts\",\n",
    "    \"eating hotdog\",\n",
    "    \"eating ice cream\",\n",
    "    \"fixing hair\",\n",
    "    \"hammer throw\",\n",
    "    # will regards all kicking as simply \"kicking\"\n",
    "    \"high kick\",\n",
    "    # will regards jogging and running on treadmill as \"running\"\n",
    "    \"jogging\",\n",
    "    \"laughing\",\n",
    "    \"mopping floor\",\n",
    "    \"moving furniture\",\n",
    "    \"opening bottle\",\n",
    "    \"plastering\",\n",
    "    # will regards all punching as simply \"punching\"\n",
    "    \"punching bag\",\n",
    "    \"punching person (boxing)\",\n",
    "    \"pushing cart\",\n",
    "    # will regard all readings as simply \"reading\"\n",
    "    \"reading book\",\n",
    "    \"reading newspaper\",\n",
    "    \"rock scissors paper\",\n",
    "    \"running on treadmill\",\n",
    "    \"shaking hands\",\n",
    "    \"shaking head\",\n",
    "    \"side kick\",\n",
    "    \"slapping\",\n",
    "    \"smoking\",\n",
    "    \"sneezing\",\n",
    "    \"spray painting\",\n",
    "    \"spraying\",\n",
    "    \"stretching arm\",\n",
    "    \"stretching leg\",\n",
    "    \"sweeping floor\",\n",
    "    \"swinging legs\",\n",
    "    \"texting\",\n",
    "    # will regards all throwing as simply \"throwing\"\n",
    "    \"throwing axe\",\n",
    "    \"throwing ball\",\n",
    "    \"unboxing\",\n",
    "    \"unloading truck\",\n",
    "    \"using computer\",\n",
    "    \"using remote controller (not gaming)\",\n",
    "    \"welding\",\n",
    "    \"writing\",\n",
    "    \"yawning\",\n",
    "]\n",
    "len(REL_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8fbed9a8-a545-4507-b8c5-eef9bf1226f1"
    }
   },
   "source": [
    "### Load model to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a81a98ce-278d-40e2-b87b-83a8e3f954c1"
    }
   },
   "outputs": [],
   "source": [
    "if cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "62364004-0d66-4d4a-8c8e-b1fc83049a03"
    }
   },
   "source": [
    "## Run Model\n",
    "Here, we use a sliding window classification for action recognition on the continuous webcam stream. We average the last 5 windows results to smoothing out the prediction results. We also reject classes that the score is less than `SCORE_THRESHOLD`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e5e52116-74fc-47fa-be98-aa6d4e5fe58d"
    }
   },
   "outputs": [],
   "source": [
    "SCORE_THRESHOLD = 0.04\n",
    "AVERAGING_SIZE = 5  # Averaging 5 latest clips to make video-level prediction (or smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "d36b8812-e5a4-4589-90fc-7884a6d71bd7"
    }
   },
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    transforms.ToTensorVideo(),\n",
    "    transforms.ResizeVideo(IM_SCALE),\n",
    "    transforms.CenterCropVideo(INPUT_SIZE),\n",
    "    transforms.NormalizeVideo(MEAN, STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "bd147e54-4445-491c-ac40-6ff51383c39f"
    }
   },
   "outputs": [],
   "source": [
    "def predict(frames, transform, device, model):\n",
    "    clip = torch.from_numpy(np.array(frames))\n",
    "    # Transform frames and append batch dim\n",
    "    sample = torch.unsqueeze(transform(clip), 0)\n",
    "    sample = sample.to(device)\n",
    "    output = model(sample)\n",
    "    scores = nn.functional.softmax(output, dim=1).data.cpu().numpy()[0]\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def filter_labels(\n",
    "    id_score_dict,\n",
    "    labels,\n",
    "    threshold=0.0,\n",
    "    target_labels=None,\n",
    "    filter_labels=None\n",
    "):\n",
    "    # Show only interested actions (target_labels) with a confidence score >= threshold\n",
    "    result = {}\n",
    "    for i, s in id_score_dict.items():\n",
    "        l = labels[i]\n",
    "        if (s < threshold) or\\\n",
    "           (target_labels is not None and l not in target_labels) or\\\n",
    "           (filter_labels is not None and l in filter_labels):\n",
    "            continue\n",
    "        \n",
    "        # Simplify some labels\n",
    "        if l.startswith('eating'):\n",
    "            l = 'eating'\n",
    "        elif l.startswith('reading'):\n",
    "            l = 'reading'\n",
    "        elif l.startswith('punching'):\n",
    "            l = 'punching'\n",
    "        elif l.startswith('throwing'):\n",
    "            l = 'throwing'\n",
    "        elif l.endswith('kick'):\n",
    "            l = 'kicking'\n",
    "        elif l == 'jogging' or l == 'running on treadmill':\n",
    "            l = 'running'\n",
    "\n",
    "        if l in result:\n",
    "            result[l] += s\n",
    "        else:\n",
    "            result[l] = s\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "29de00c8-4259-40e0-909b-e0f6a96950e4"
    }
   },
   "source": [
    "### On Webcam Stream\n",
    "#### Start webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6899ba96-ed4c-4716-9113-1706685fcb5b"
    }
   },
   "outputs": [],
   "source": [
    "# Webcam\n",
    "w_cam = CameraStream(\n",
    "    constraints={\n",
    "        'facing_mode': 'user',\n",
    "        'audio': False,\n",
    "        'video': {'width': 400, 'height': 400}\n",
    "    },\n",
    "    layout=Layout(width='400px')\n",
    ")\n",
    "\n",
    "# Image recorder for taking a snapshot\n",
    "w_imrecorder = ImageRecorder(\n",
    "    format='jpg',\n",
    "    stream=w_cam,\n",
    "    layout=Layout(padding='0 0 0 100px')\n",
    ")\n",
    "\n",
    "# Text widget to show our classification results\n",
    "w_text = HTML(layout=Layout(padding='0 0 0 100px'))\n",
    "\n",
    "def predict_webcam_frames():\n",
    "    \"\"\" Predict activity by using a pretrained model\n",
    "    \"\"\"\n",
    "    global w_imrecorder, w_text, is_playing\n",
    "    global device, model\n",
    "    \n",
    "    # Use deque for sliding window over frames\n",
    "    window = deque()\n",
    "    scores_cache = deque()\n",
    "    scores_sum = np.zeros(NUM_CLASSES)\n",
    "    \n",
    "    while is_playing:\n",
    "        try:\n",
    "            # Get the image (RGBA) and convert to RGB\n",
    "            im = Image.open(\n",
    "                io.BytesIO(w_imrecorder.image.value)\n",
    "            ).convert('RGB')     \n",
    "            window.append(np.array(im))\n",
    "            if len(window) == NUM_FRAMES:\n",
    "                # Make a prediction\n",
    "                t = time()\n",
    "                scores = predict(window, transform, device, model)\n",
    "                dur = time() - t\n",
    "                # Averaging scores across clips (dense prediction)\n",
    "                scores_cache.append(scores)\n",
    "                scores_sum += scores\n",
    "                if len(scores_cache) == AVERAGING_SIZE:\n",
    "                    scores_avg = scores_sum / AVERAGING_SIZE\n",
    "                    # 1. Pick top-5 labels \n",
    "                    top5_id_score_dict = {\n",
    "                        i: scores_avg[i] for i in (-scores_avg).argpartition(4)[:5]\n",
    "                    }\n",
    "                    # 2. Filter by SCORE_THRESHOLD and REL_LABELS\n",
    "                    top5_label_score_dict = filter_labels(\n",
    "                        top5_id_score_dict,\n",
    "                        labels,\n",
    "                        threshold=SCORE_THRESHOLD,\n",
    "                        target_labels=REL_LABELS\n",
    "                    )\n",
    "                    # 3. Display the labels sorted by scores\n",
    "                    top5 = sorted(\n",
    "                        top5_label_score_dict.items(), key=lambda kv: -kv[1]\n",
    "                    )\n",
    "                    # Plot final results nicely\n",
    "                    w_text.value = (\n",
    "                        \"{} fps<p style='font-size:20px'>\".format(1//dur) + \"<br>\".join([\n",
    "                            \"{} ({:.3f})\".format(k, v) for k, v in top5\n",
    "                        ]) + \"</p>\"\n",
    "                    )\n",
    "                    scores_sum -= scores_cache.popleft()\n",
    "                window.popleft()\n",
    "            else:\n",
    "                w_text.value = \"Preparing...\"     \n",
    "        except OSError:\n",
    "            # If im_recorder doesn't have valid image data, skip it. \n",
    "            pass\n",
    "        except BaseException as e:\n",
    "            w_text.value = \"Exception: \" + str(e)\n",
    "            break\n",
    "\n",
    "        # Taking the next snapshot programmatically\n",
    "        w_imrecorder.recording = True\n",
    "        sleep(0.02)\n",
    "\n",
    "is_playing = False\n",
    "#  Once prediciton started, hide image recorder widget for faster fps\n",
    "def start(_):\n",
    "    global is_playing\n",
    "    # Make sure this get called only once\n",
    "    if not is_playing:\n",
    "        w_imrecorder.layout.display = 'none'\n",
    "        is_playing = True\n",
    "        Thread(target=predict_webcam_frames).start()\n",
    "    \n",
    "w_imrecorder.image.observe(start, 'value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HBox([w_cam, w_imrecorder, w_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3b2047f3-87a7-47bd-a53c-2fa90540be99"
    }
   },
   "source": [
    "To start inference on webcam stream, click 'capture' button when the stream is started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "acf10288-9c35-45ec-8b20-cbb3782dead3"
    }
   },
   "source": [
    "#### Stop Webcam and clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a218abeb-4dd9-4030-9e54-550415ae8a89"
    }
   },
   "outputs": [],
   "source": [
    "is_playing = False\n",
    "Widget.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3b3f708a-26d1-4a0c-bf1c-86fcf842751b"
    }
   },
   "source": [
    "### Appendix: Run on a video file\n",
    "Here, we show how to use the model on a video file. We utilize threading so that the inference does not block the video preview.\n",
    "* Prerequisite - Download HMDB51 video files from [here](http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/#Downloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "8ef2c7ed-365b-4507-be57-db36ad839380"
    }
   },
   "outputs": [],
   "source": [
    "def _predict_video_frames(window, scores_cache, scores_sum, is_ready):\n",
    "    t = time()\n",
    "    scores = predict(window, transform, device, model)\n",
    "    dur = time() - t\n",
    "    # Averaging scores across clips (dense prediction)\n",
    "    scores_cache.append(scores)\n",
    "    scores_sum += scores\n",
    "    if len(scores_cache) == AVERAGING_SIZE:\n",
    "        scores_avg = scores_sum / AVERAGING_SIZE\n",
    "        top5_id_score_dict = {\n",
    "            i: scores_avg[i] for i in (-scores_avg).argpartition(4)[:5]\n",
    "        }\n",
    "        top5_label_score_dict = filter_labels(\n",
    "            top5_id_score_dict,\n",
    "            labels, \n",
    "            threshold=SCORE_THRESHOLD,\n",
    "        )\n",
    "        top5 = sorted(top5_label_score_dict.items(), key=lambda kv: -kv[1])\n",
    "        # Plot final results nicely\n",
    "        d_caption.update(IPython.display.HTML(\n",
    "            \"{} fps<p style='font-size:20px'>\".format(1 // dur) + \"<br>\".join([\n",
    "                \"{} ({:.3f})\".format(k, v) for k, v in top5\n",
    "            ]) + \"</p>\"\n",
    "        ))\n",
    "        scores_sum -= scores_cache.popleft()\n",
    "    \n",
    "    # Inference done. Ready to run on the next frames.\n",
    "    window.popleft()\n",
    "    is_ready[0] = True\n",
    "\n",
    "def predict_video_frames(video_filepath, d_video, d_caption):\n",
    "    \"\"\"Load video and show frames and inference results on\n",
    "    d_video and d_caption displays\n",
    "    \"\"\"\n",
    "    video_reader = decord.VideoReader(video_filepath)\n",
    "    print(\"Total frames = {}\".format(len(video_reader)))\n",
    "    \n",
    "    is_ready = [True]\n",
    "    window = deque()\n",
    "    scores_cache = deque()\n",
    "    scores_sum = np.zeros(NUM_CLASSES)\n",
    "    while True:\n",
    "        try:\n",
    "            frame = video_reader.next().asnumpy()\n",
    "            if len(frame.shape) != 3:\n",
    "                break\n",
    "            \n",
    "            # Start an inference thread when ready\n",
    "            if is_ready[0]:\n",
    "                window.append(frame)\n",
    "                if len(window) == NUM_FRAMES:\n",
    "                    is_ready[0] = False\n",
    "                    Thread(\n",
    "                        target=_predict_video_frames,\n",
    "                        args=(window, scores_cache, scores_sum, is_ready)\n",
    "                    ).start()\n",
    "                    \n",
    "            # Show video preview\n",
    "            f = io.BytesIO()\n",
    "            im = Image.fromarray(frame)\n",
    "            im.save(f, 'jpeg')\n",
    "\n",
    "            d_video.update(IPython.display.Image(data=f.getvalue()))\n",
    "            sleep(0.03)\n",
    "        except:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "bb7ddeb7-86b4-4c11-b600-205be2aa2bb5"
    }
   },
   "outputs": [],
   "source": [
    "video_filepath = os.path.join(\n",
    "    \"data\", \"hmdb51\", \"videos\",\n",
    "    \"push\", \"Baby_Push_Cart_push_f_cm_np1_ri_bad_0.avi\"\n",
    ")\n",
    "\n",
    "d_video = IPython.display.display(\"\", display_id=1)\n",
    "d_caption = IPython.display.display(\"Preparing...\", display_id=2)\n",
    "\n",
    "try:\n",
    "    predict_video_frames(video_filepath, d_video, d_caption)\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "dce4df92-8dd0-430a-b27a-b44538dfd31e"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r2p1d",
   "language": "python",
   "name": "r2p1d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
